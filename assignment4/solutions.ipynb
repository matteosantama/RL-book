{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241: Assigment 4\n",
    "\n",
    "## Question 3\n",
    "\n",
    "The state space of this MDP is $\\mathcal{S} = \\{s \\mid s = 1, 2, ..., n, n+1\\}$ where $1 \\leq s \\leq n$ are states for each of the $n$ jobs, and $s = n + 1$ is the unemployed state.\n",
    "\n",
    "The action space can be defined as $\\mathcal{A} = \\{c, r\\}$ where $c$ represents accepting a new job and $r$ represents rejecting a new job.\n",
    "\n",
    "The transition function $\\mathcal{P}(s, a, s')$ can be computed as $\\mathbb{P}\\left[S_{t+1} = s' \\mid S_t = s, A = a \\right]$.\n",
    "\n",
    "$$\\mathcal{P}(s, a, s) = 1 - \\alpha : \\forall s \\neq n + 1, a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{P}(s, a, s') = 0 : \\forall s \\neq s', s' \\neq n + 1, a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{P}(s, a, n + 1) = \\alpha : \\forall s \\neq n + 1, a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{P}(n + 1, c, s') = p_{s'} : \\forall s' \\neq n + 1$$\n",
    "$$\\mathcal{P}(n + 1, r, s')= 0.0 : \\forall s' \\neq n + 1$$\n",
    "$$\\mathcal{P}(n + 1, r, n + 1) = 1.0$$\n",
    "$$\\mathcal{P}(n + 1, c, n + 1)= 0.0$$\n",
    "\n",
    "The reward function $\\mathcal{R}(s, a)$ is:\n",
    "\n",
    "$$\\mathcal{R}(s, a) = \\log(w_s) : \\forall s \\neq n + 1, \\forall a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{R}(n + 1, c) = \\log \\left(\\mathbb{E}\\left[w_{s'} \\mid S_{t+1} = s'\\right]\\right) = \\log \\left(\\underset{1 \\leq i \\leq n}{\\sum}p_i w_i \\right)$$\n",
    "$$\\mathcal{R}(n + 1, r) = \\log(w_{0})$$\n",
    "\n",
    "Now we can write the general Bellman Optimality Equation:\n",
    "\n",
    "$$ \\pmb{V}^*(s) = \\underset{a \\in \\mathcal{A}}{\\max}\\left\\{ \\mathcal{R}(s, a) \n",
    "    + \\gamma \\cdot \\underset{s' \\in \\mathcal{S}}{\\sum} \\mathcal{P}(s, a, s') \\cdot \\pmb{V}^*(s') \\right\\}\n",
    "$$\n",
    "\n",
    "and solve for $\\pmb{V}^*$ with the Value Iteration algorithm. From there, we can back out $\\pi^*$, the optimal (deterministic) policy.\n",
    "\n",
    "$$\\pmb{Q}^*(s,a) = \\mathcal{R}(s,a) + \\gamma \\cdot \\underset{s' \\in \\mathcal{S}}{\\sum}\\mathcal{P}(s, a, s') \\cdot \\pmb{V}^*(s')$$\n",
    "$$\\pi^*(s) = \\underset{a \\in \\mathcal{A}}{\\arg \\max} \\left\\{ \\pmb{Q}^*(s,a) \\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import (\n",
    "    Mapping,\n",
    "    List, \n",
    "    Generic, \n",
    "    TypeVar, \n",
    "    Set\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar(\"S\")\n",
    "\n",
    "@dataclass\n",
    "class WageMaximizer(Generic[S]):\n",
    "    \"\"\"\n",
    "    Solves the Wage-Utility Maximization problem via the \n",
    "    Bellman Optimality Equation.\n",
    "    \"\"\"\n",
    "    gamma: float\n",
    "    alpha: float\n",
    "\n",
    "    employed_states: List[S]\n",
    "    unemployed_state: S\n",
    "    \n",
    "    employed_wages: np.array\n",
    "    unemployed_wage: float\n",
    "        \n",
    "    probs: np.array\n",
    "        \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Some more set up.\"\"\"\n",
    "        self._validate()\n",
    "        self.actions: Set[str] = {'c', 'r'}\n",
    "        self._state_probs = {\n",
    "            s: p for s, p in \n",
    "            zip(self.employed_states, self.probs)\n",
    "        }\n",
    "        self._state_wages = {\n",
    "            s: r for s, r in \n",
    "            zip(self.employed_states, self.employed_wages)\n",
    "        }\n",
    "        \n",
    "    def _validate(self) -> None:\n",
    "        \"\"\"Validate given problem parametersl.\"\"\"\n",
    "        a = len(self.employed_wages)\n",
    "        b = len(self.probs)\n",
    "        c = len(self.employed_states)\n",
    "        if not (a == b == c):\n",
    "            raise ValueError(\"Check parameter lengths.\")\n",
    "        if any(x <= 0 for x in self.employed_wages + [self.unemployed_wage]):\n",
    "            raise ValueError(\"Must have positive wages.\")\n",
    "        if self.probs.sum() != 1:\n",
    "            raise ValueError(\"Check transition probabilities.\")\n",
    "        if self.unemployed_state in self.employed_states:\n",
    "            raise ValueError(\n",
    "                f\"{self.unemployed_state} cannot also be an employed_state\"\n",
    "            )\n",
    "    \n",
    "    @property\n",
    "    def states(self) -> List[S]:\n",
    "        return self.employed_states + [self.unemployed_state]\n",
    "    \n",
    "    def P(self, state: S, action: str, next_state: S) -> float:\n",
    "        \"\"\"Return the (s, a, s') transition probability\"\"\"\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f\"{a=} is an invalid action\")\n",
    "        m: S = self.unemployed_state\n",
    "        if (state, action, next_state) == (m, 'c', m):\n",
    "            return 0\n",
    "        if (state, action, next_state) == (m, 'r', m):\n",
    "            return 1\n",
    "        if (state, action) == (m, 'r'):\n",
    "            return 0\n",
    "        if (state, action) == (m, 'c'):\n",
    "            return self._state_probs[next_state]\n",
    "        if next_state == m:\n",
    "            return self.alpha\n",
    "        if state == next_state:\n",
    "            return 1 - self.alpha\n",
    "        return 0\n",
    "    \n",
    "    def R(self, state: S, action: str) -> float:\n",
    "        \"\"\"Return the expected reward of (s, a).\"\"\"\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f\"{a=} is an invalid action\")\n",
    "        m: S = self.unemployed_state\n",
    "        if (state, action) == (m, 'r'):\n",
    "            return np.log(self.unemployed_wage)\n",
    "        if (state, action) == (m, 'c'):\n",
    "            return np.log(self.employed_wages @ self.probs)\n",
    "        return np.log(self._state_wages[state])\n",
    "    \n",
    "    def value_iteration(self) -> np.array:\n",
    "        \"\"\"Compute the optimal value function.\"\"\"\n",
    "        vk = np.zeros(len(self.states))\n",
    "        \n",
    "        def maximize(s: S, vfunc: np.array) -> float:\n",
    "            \"\"\"Maximize value function for a state over actions.\"\"\"\n",
    "            maximum = float(\"-inf\")\n",
    "            for a in self.actions:\n",
    "                _sum = sum(\n",
    "                    self.P(s, a, s_) * vfunc[j] \n",
    "                    for j, s_ in enumerate(self.states)\n",
    "                )\n",
    "                val = self.R(s, a) + self.gamma * _sum\n",
    "                maximum = max(val, maximum)\n",
    "            return maximum\n",
    "        \n",
    "        while True:\n",
    "            improvement = vk.copy()\n",
    "            for i, state in enumerate(self.states):\n",
    "                improvement[i] = maximize(state, vk)\n",
    "            if np.linalg.norm(improvement - vk) < 1e-5:\n",
    "                return {s: v for s, v in zip(solver.states, improvement)}\n",
    "            vk = improvement\n",
    "            \n",
    "    def find_optimal_policy(self) -> Mapping[S, str]:\n",
    "        \"\"\"Find the an optimal deterministic policy.\"\"\"\n",
    "        pi = {}\n",
    "        v_star = self.value_iteration()\n",
    "        \n",
    "        def q_star(s: S, a: str) -> float:\n",
    "            \"\"\"Compute Q^* for a (state, action) pair.\"\"\"\n",
    "            return (self.R(s, a) + self.gamma * \n",
    "                    sum(self.P(s, a, s_) * v_star[s_] \n",
    "                        for s_ in self.states))\n",
    "        \n",
    "        for s in self.states:\n",
    "            _max = float(\"-inf\")\n",
    "            action = None\n",
    "            for a in self.actions:\n",
    "                if q_star(s, a) > _max:\n",
    "                    _max = q_star(s, a)\n",
    "                    action = a\n",
    "            pi[s] = action\n",
    "        return pi\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.7760895874094604,\n",
       " 2: 0.014389425423822473,\n",
       " 3: 1.5377897493950983,\n",
       " 4: 2.1527521243062777,\n",
       " 5: 1.221655618964289,\n",
       " 6: 1.3094432712224158}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma: float = 0.1\n",
    "alpha: float = 0.1\n",
    "\n",
    "employed_states: List[int] = [1, 2, 3, 4, 5]\n",
    "unemployed_state: int = 6\n",
    "\n",
    "employed_wages: List[float] = [2, 1, 4, 7, 3]\n",
    "unemployed_wage: float = 1\n",
    "    \n",
    "probs: List[float] = [0.1, 0.2, 0.4, 0.1, 0.2]\n",
    "    \n",
    "solver = WageMaximizer[int](\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    employed_states=employed_states,\n",
    "    unemployed_state=unemployed_state,\n",
    "    employed_wages=np.array(employed_wages),\n",
    "    unemployed_wage=unemployed_wage,\n",
    "    probs=np.array(probs)\n",
    ")\n",
    "v_star = solver.value_iteration()\n",
    "v_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'r', 2: 'r', 3: 'r', 4: 'r', 5: 'r', 6: 'c'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that for all the 'employed' states, it doesn't matter what our\n",
    "# policy is because our action does not affect the transition\n",
    "# probabilities\n",
    "opt_policy = solver.find_optimal_policy()\n",
    "opt_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
