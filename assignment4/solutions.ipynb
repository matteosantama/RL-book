{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241 -- Assigment 4\n",
    "\n",
    "## Question 2: Frog Croaking Revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Categorical, Choose, Constant\n",
    "from rl.markov_process import StateReward\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.dynamic_programming import value_iteration, policy_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frog_mdp(n: int) -> FiniteMarkovDecisionProcess:\n",
    "    \"\"\"Construct the frog-problem MDP for a given n.\"\"\"\n",
    "    \n",
    "    rewards = {s: 0 for s in range(n)}\n",
    "    rewards[n] = +1\n",
    "    \n",
    "    def _get_state_rewards(state: int, action: str) -> StateReward:\n",
    "        \"\"\"Return the distribution of rewards.\"\"\"\n",
    "        if action == 'A':\n",
    "            return Categorical({\n",
    "                (state - 1, rewards[state - 1]): state / n,\n",
    "                (state + 1, rewards[state + 1]): (n - state) / n\n",
    "            })\n",
    "        if action == 'B':\n",
    "            return Choose({\n",
    "                (i, rewards[i]) for i in range(n + 1) if i != state\n",
    "            })\n",
    "        raise RuntimeError(f\"Uh oh, should not be reaching here!\")\n",
    "    \n",
    "    actions = 'AB'\n",
    "    state_action_mapping = {0: None, n: None}\n",
    "    for s in range(1, n):\n",
    "        state_action_mapping[s]= {}\n",
    "        for a in actions:\n",
    "            state_action_mapping[s][a] = _get_state_rewards(s, a)\n",
    "    return FiniteMarkovDecisionProcess(state_action_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [n for n in range(20, 120, 10)]\n",
    "vi_steps = []\n",
    "pi_steps = []\n",
    "\n",
    "for n in ns:\n",
    "    mdp = get_frog_mdp(n)\n",
    "\n",
    "    # first we see how many steps it takes value iteration to converge\n",
    "    old = np.random.randn(n - 1)\n",
    "    for i, v in enumerate(value_iteration(mdp, gamma=1.0)):\n",
    "        solution = np.array([v[j] for j in range(1, n)])\n",
    "        if np.linalg.norm(solution - old) < 1e-5:\n",
    "            vi_steps.append(i)\n",
    "            break\n",
    "        old = solution\n",
    "\n",
    "    # then we try policy iteration\n",
    "    old = np.random.randn(n - 1)\n",
    "    for i, (v, p) in enumerate(policy_iteration(mdp, gamma=1.0)):\n",
    "        solution = np.array([v[j] for j in range(1, n)])\n",
    "        if np.linalg.norm(solution - old) < 1e-5:\n",
    "            pi_steps.append(i)\n",
    "            break\n",
    "        old = solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXgV5fn/8fedPSRhDwgECJsga8CIKKBs7gioCAJlUSoV3GpbLfXbVrr4++K3VgigUuqGS0WFotSlCgEE3MOOgCRAgIQlIUggrIbcvz9mcgwxCQfIyUly7td15cqZOTNz7kxOzifzPDPPiKpijDHGAAT5uwBjjDGVh4WCMcYYDwsFY4wxHhYKxhhjPCwUjDHGeIT4u4CLUb9+fY2Pj/d3GcYYU6WsXr36oKrGlvRclQ6F+Ph4UlJS/F2GMcZUKSKyq7TnrPnIGGOMh4WCMcYYDwsFY4wxHlW6T6EkP/zwAxkZGZw8edLfpRgvRUREEBcXR2hoqL9LMSbgVbtQyMjIICYmhvj4eETE3+WYc1BVcnJyyMjIoEWLFv4ux5iAV+2aj06ePEm9evUsEKoIEaFevXp2ZGdMJVHtQgGwQKhi7PdlTOVRLUPBGGOqq++PneZvH28l/eAxn2zfQqGc9e3bl48//visedOnT2fixImlrtOnT59yuQhv+fLlDBw40PP4888/v+htFkpPT+df//qXZzolJYWHHnqo3LZvjCnboWOneeq/W+n11FKeW76dlanZPnkdn4aCiKSLyEYRWSciKe68uiKyWERS3e913PkiIjNEJE1ENohIN1/W5isjRoxg3rx5Z82bN28eI0aMqNA6LiQU8vPzS32ueCgkJiYyY8aMC67PGOOdnLxTTP3ICYPZn26nb7sGfPzLaxh9VbxPXq8ijhT6qmqCqia605OBZFVtAyS70wA3AW3crwnA8xVQW7kbOnQoH3zwAadPnwacD9O9e/fSu3dvJk6cSGJiIh06dOCJJ54ocf3o6GjP4/nz5zNu3DgAsrOzueOOO7jiiiu44oor+Oyzz0qtIT09ndmzZzNt2jQSEhJYuXJlqetPmTKF0aNH07NnT0aPHk16ejq9e/emW7dudOvWzRMskydPZuXKlSQkJDBt2rSzjkoOHTrEkCFD6Ny5Mz169GDDhg2ebd9zzz306dOHli1bWogYcx4O5p3ifz/cQq+nlvGPFdsZcFlDPvnlNcwa2Y1LG8b47HX9cUrqYKCP+3gusBz4rTv/VXXuD/qliNQWkUaquu9CX+hP//mWzXuPXGS5Z2vfuCZP3Nqh1Ofr1q1L9+7d+eijjxg8eDDz5s1j2LBhiAhPPvkkdevW5cyZM/Tv358NGzbQuXNnr1734Ycf5pFHHqFXr17s3r2bG264gS1btpS4bHx8PPfddx/R0dH85je/AWDkyJGlrr9582ZWrVpFZGQkx48fZ/HixURERJCamsqIESNISUlh6tSpPP3007z//vuAcyRS6IknnqBr1668++67LF26lDFjxrBu3ToAtm7dyrJlyzh69Cht27Zl4sSJdj2CMWXIPnqKOSu28/qXuzmVf4ZBXRrzQL82tG4Qfe6Vy4GvQ0GBT0REgX+o6hygYZEP+v1AQ/dxE2BPkXUz3HlnhYKITMA5kqBZs2Y+LP3CFTYhFYbCiy++CMDbb7/NnDlzyM/PZ9++fWzevNnrUFiyZAmbN2/2TB85coS8vLyzjiwuZH2AQYMGERkZCTgX/z3wwAOsW7eO4OBgtm3bds5tr1q1igULFgDQr18/cnJyOHLECeNbbrmF8PBwwsPDadCgAQcOHCAuLs6rmo0JJFlHT/KPT3fwxle7OJ1fwOCEJjzQrzWtYismDAr5OhR6qWqmiDQAFovI1qJPqqq6geE1N1jmACQmJpa5bln/0fvS4MGDeeSRR1izZg3Hjx/n8ssvZ+fOnTz99NN888031KlTh3HjxpV4bn7R0zOLPl9QUMCXX35JRETEBdVU1vpRUVGex9OmTaNhw4asX7+egoKCC369QuHh4Z7HwcHBZfZbGBOIso6cZLYbBj+cKWBI1yY80Lc1LSs4DAr5tE9BVTPd71nAQqA7cEBEGgG437PcxTOBpkVWj3PnVTnR0dH07duXe+65x9PBfOTIEaKioqhVqxYHDhzgo48+KnHdhg0bsmXLFgoKCli4cKFn/vXXX8/MmTM904XNM6WJiYnh6NGj571+bm4ujRo1IigoiNdee40zZ86UuL2ievfuzRtvvAE4zUr169enZs2aZdZnTKA7cOQkUxZ9S+//W8bcL9K5tUtjlv66D88MS/BbIIAPQ0FEokQkpvAxcD2wCVgEjHUXGwu85z5eBIxxz0LqAeReTH+Cv40YMYL169d7QqFLly507dqVdu3aMXLkSHr27FnielOnTmXgwIFcffXVNGrUyDN/xowZpKSk0LlzZ9q3b8/s2bPLfP1bb72VhQsXejqavV1/0qRJzJ07ly5durB161bPUUTnzp0JDg6mS5cuTJs27ax1pkyZwurVq+ncuTOTJ09m7ty5Xu8nYwLN/tyTPPHeJnr/3zJe+3IXg7o0Zumvr+XpO7sQXz/q3BvwMXH6dX2wYZGWOEcH4DRT/UtVnxSResDbQDNgFzBMVQ+J024yC7gROA7craplnryfmJioxc/v37JlC5dddln5/jDG5+z3Zqq7fbkneH75duZ9vYcCVe7oFsf9fVvTrF6NCq9FRFYXOSP0LD7rU1DVHUCXEubnAP1LmK/A/b6qxxhj/GHv4RM8tzyNt7/JoECVOxPjmNSnNU3rVnwYeKPajZJqjDGVQebhEzy3LI23U5yTKode3pRJfVpV2jAoZKFgjDHlaM+h4zy3fDvzVzthMCyxKZP6tqZJ7Ug/V+YdCwVjjCkHew4d59llacxfnUGQCHdd0YyJfVrRuIqEQSELBWOMuQi7c5wwWLDGCYORVzph0KhW1QqDQhYKxhhzAXblHGPW0jT+vTaT4CDhZz2ac9+1rbik1sVd8OlvNnS2DwQHB5OQkEDHjh258847OX78eKnLvvLKKzzwwAMAzJ49m1dfffWiXjs9PZ2OHTsCzgVqH3744UVtr6jDhw/z3HPPeab37t3L0KFDy237xlQF6QeP8eu319Pv75+yaP1exlzVnJWP9WXKoA5VPhDAQsEnIiMjWbduHZs2bSIsLOycF5oVuu+++xgzZky51XEhoVDWMBTFQ6Fx48bMnz//guszpirZkZ3Hr95aR7+/L+f9DXsZe1U8Kx/ryxO3dqBhzaofBoUsFHysd+/epKWllTq8dFFTpkzh6aefBiAtLY0BAwbQpUsXunXrxvbt2xkzZgzvvvuuZ/lRo0bx3nvv/WQ7AKdPn+aPf/wjb731FgkJCbz11lscO3aMe+65h+7du9O1a1fPuq+88gqDBg2iX79+9O/fn7y8PPr370+3bt3o1KmTZ7nJkyezfft2EhISePTRR886Kjl58iR33303nTp1omvXrixbtsyz7dtvv50bb7yRNm3a8Nhjj5XfzjWmAmzPzuORt9Yx4JlP+XDTPu7p2YKVv+3LH29tT4NqFAaFqnefwkeTYf/G8t3mJZ3gpqleLZqfn89HH33EjTfeWObw0iUZNWoUkydP5rbbbuPkyZMUFBQwfvx4pk2bxpAhQ8jNzeXzzz8vdUiJsLAw/vznP5OSksKsWbMAePzxx+nXrx8vvfQShw8fpnv37gwYMACANWvWsGHDBurWrUt+fj4LFy6kZs2aHDx4kB49ejBo0CCmTp3Kpk2bPHWnp6d7Xu/ZZ59FRNi4cSNbt27l+uuv94ywum7dOtauXUt4eDht27blwQcfpGnTphhTmaVl5TFzaSr/Wb+X8JBgft67Jff2bklsTPi5V67Cqnco+MmJEydISEgAnCOF8ePHc+WVV5Y6vHRxR48eJTMzk9tuuw3AM1Lptddey6RJk8jOzmbBggXccccdhIR4/yv85JNPWLRokedo5OTJk+zevRuA6667jrp16wKgqjz++OOsWLGCoKAgMjMzOXDgQJnbXrVqFQ8++CAA7dq1o3nz5p5Q6N+/P7Vq1QKgffv27Nq1y0LBVFqpB44yY2ka72/YS0RIMPf2bsm917SkfnT1DoNC1TsUvPyPvrwV9in4wpgxY3j99deZN28eL7/88nmtq6osWLCAtm3bnjX/q6++Omv47DfeeIPs7GxWr15NaGgo8fHxJQ7z7S0bPttUBdsOHGVGciofbNxHZGgwv7imFff2bkG9AAmDQtanUEHOZ3jpmJgY4uLiPP0Hp06d8pzBNG7cOKZPnw44/3WXpfhw1zfccAMzZ86kcBDEtWvXlrhebm4uDRo0IDQ0lGXLlrFr164St1faz7dt2zZ27979k/AxpjLauv8I97+xhhumr2DZ1iwmXtuKVb/tx+Sb2gVcIICFQoU53+GlX3vtNWbMmEHnzp25+uqr2b9/P+Dcb+Gyyy7j7rvvPudr9u3bl82bN3s6mv/whz/www8/0LlzZzp06MAf/vCHEtcbNWoUKSkpdOrUiVdffZV27doBUK9ePXr27EnHjh159NFHz1pn0qRJFBQU0KlTJ4YPH84rr7xy1hGCMZXNln1HmPj6am6cvpJPt2UzqY8TBo/d2I66UWH+Ls9vfDZ0dkUIxKGzjx8/TqdOnVizZo2nnb46qO6/N1N5bN57hBnJqfz32/3EhIcwrmc843u1oHaNwAkCvwydbcrfkiVLGD9+PI888ki1CgRjKsKmzFxmJKfyyeYDxISH8FD/Nozv2YJaNUL9XVqlYqFQhQwYMMDTvm+M8c6mzFymL0llyZYDxESE8HD/NtxjYVCqahkKqopzIzdTFVTlJkxTeW3IOEzSklSSt2ZRMyKERwZcyrie8dSKtDAoS7ULhYiICHJycqhXr54FQxWgquTk5HiuxTDmYq3fc5ik5FSWbs2iVmQov77uUsb2jKdmhIWBN6pdKMTFxZGRkUF2dra/SzFeioiIIC4uzt9lmCpu7e7vSUpOZfl32dSuEcpvrr+UsVfHE2NhcF6qXSiEhobSokULf5dhjKkgq3c5YbBiWzZ1aoTy6A1tGXt1PNHh1e7jrULYXjPGVEmrdx1i+pJUVqYepG5UGL+9sR2jr2puYXCRbO8ZY6qUb9IPkbQklVVpB6kXFcbvbmrHz3o0J8rCoFzYXjTGVAlf7cghKTmVz7fnUD86jMdvdsKgRph9jJUn25vGmErtyx05JC1J5YsdOdSPDuf3t1zGqCubExkW7O/SqiULBWNMpaOqfOGGwVc7DxEbY2FQUSwUjDGVhqryxfYcpi9J5ev0QzSICeePA9sz8spmRIRaGFQECwVjjN+pKp+l5ZCUvI1v0r+nYc1wptzanru6WxhUNAsFY4zfqCorUw+SlJzK6l3fc0nNCP48uAPDEptaGPiJhYIxpsKpKitSDzJ9yTbW7j5Mo1oR/GVwB4Zd0ZTwEAsDf7JQMMZUGFVl+bZskpaksm7PYRrXiuCvQzpyZ2KchUEl4fNQEJFgIAXIVNWBItICmAfUA1YDo1X1tIiEA68ClwM5wHBVTfd1fcYY31NVln2XRVJyGuv3HKZJ7Uj+322dGHp5HGEhdgPIyqQijhQeBrYAhTckfgqYpqrzRGQ2MB543v3+vaq2FpG73OWGV0B9xhgfUVWWbs0iKTmVDRm5xNWJZOrtnbi9m4VBZeXTUBCROOAW4EngV+KMZd0PGOkuMheYghMKg93HAPOBWSIiaoPtG1PlqCpLtmQxIzmVjZm5NK0byVN3OGEQGmxhUJn5+khhOvAYEONO1wMOq2q+O50BNHEfNwH2AKhqvojkussfLLpBEZkATABo1qyZT4s3xpwfVeWTzQeYkZzKt3uP0LxeDf5vaGdu69rEwqCK8FkoiMhAIEtVV4tIn/LarqrOAeYAJCYm2lGEMZVAQYETBknJqWzZ54TB03d2YUhCY0IsDKoUXx4p9AQGicjNQAROn0ISUFtEQtyjhTgg010+E2gKZIhICFALp8PZGFNJFRQoH3+7n6TkVLbuP0qL+lH8/c4uDLYwqLJ8Fgqq+jvgdwDukcJvVHWUiLwDDMU5A2ks8J67yiJ3+gv3+aXWn2BM5VRQoHy0aT8zlzph0LJ+FNOGd+HWzhYGVZ0/rlP4LTBPRP4KrAVedOe/CLwmImnAIeAuP9RmjClDQYHy4aZ9zEhOZduBPFrGRpF0VwIDOzcmOMjuiV4dVEgoqOpyYLn7eAfQvYRlTgJ3VkQ9xpjzc6ZA+WDjPmYmp5KalUfrBtEWBtWUXdFsjCnVmQLl/Q17mbk0jbSsPNo0iGbmiK7c3KmRhUE1ZaFgjPmJAvfIYPqSbWzPPsalDaN5dmQ3bup4CUEWBtWahYIxxqPwbKLpS1L57sBRLm0YzXOjunFjBwuDQGGhYIxBVVm8+QDTljjXGbSMjWLGiK4M7NTIwiDAWCgYE8BUleXfZfPM4m1szMwlvl4Npg3vwqAuTazPIEBZKBgTgApvbvPM4m2s23OYpnUj+Zs7HIVdZxDYLBSMCSCF90B+ZvE2UnZ9T5Pazqild1xuA9UZh4WCMQHiqx1OGHy18xCX1IzgL0M6MsxubmOKsVAwpppbvesQ0xansirtILEx4Uy5tT13dW9m90A2JbJQMKaaWrfnMNMWb+PTbdnUjw7j97dcxs96NLcwMGWyUDCmmtmUmcu0xdtI3ppFnRqh/O6mdoy+qjk1wuzP3ZybvUuMqSY27z3C9CXb+GTzAWpFhvLoDW0Ze3U80eH2Z268Z+8WY6q4bQeOMn3JNj7cuJ+YiBB+dd2ljOsZT82IUH+XZqogCwVjqqi0rDySklN5f8NeosJCeKhfa8b3akmtGhYG5sJZKBhTxew8eIwZyam8ty6TiNBgJl7bint7t6ROVJi/SzPVgIWCMVXE7pzjzFyayr/XZhIaLNzbuyUTrmlJvehwf5dmqhGvQkFEegFtVPVlEYkFolV1p29LM8YAZHx/nGeXpfFOSgZBQcLYq+K5r09LGsRE+Ls0Uw2dMxRE5AkgEWgLvAyEAq8DPX1bmjGBbV/uCZ5dlsZb3+xBEEZd2YxJfVvTsKaFgfEdb44UbgO6AmsAVHWviMT4tCpjAti+3BPMXr6dN7/Zg6oyLLEp9/dtTePakf4uzQQAb0LhtKqqiCiAiET5uCZjAtLewyd4fvl23vpmDwWqDL08jvv7tqZp3Rr+Ls0EEG9C4W0R+QdQW0TuBe4B/unbsowJHJmHT/DcsjTeTtkDwNDLmzKpTysLA+MX5wwFVX1aRK4DjuD0K/xRVRf7vDJjqjmnA3k781c7YTAssSkT+7Qiro6FgfEfbzqafwW8ZUFgTPnYc+g4zy13zyYSYfgVTZnYpzVNrM/AVALeNB/FAJ+IyCHgLeAdVT3g27KMqX525zinli5Y44TByCubcd+1rawD2VQq3jQf/Qn4k4h0BoYDn4pIhqoO8Hl1xlQDu3KOMWtpGv9em0lwkHNq6X19WtGoloWBqXzO54rmLGA/kAM08E05xlQf6QePMWtZGgvXZhISJIzu0ZyJfVrZdQamUvOmT2ESMAyIBd4B7lXVzb4uzJiqaufBY8xcmsp76/YSUngF8rUtaWBhYKoAb44UmgK/VNV1vi7GmKpse3Yes5am8d66TMJCghh3dTy/uNaGozBVS6mhICJ13Yd/KzYNgKoe8mFdxlQZaVl5zFqayqL1ewkLCWJ8rxZMuKYVsTE2UJ2peso6UlgNqPtYij2nQEufVGRMFZGWdZQZyWn8Z8NeIkKCubd3S+69piX1bdRSU4WVGgqq2uJiNiwiEcAKINx9nfmq+oSItADmAfVwgme0qp4WkXDgVeBynM7s4aqafjE1GOML2w4cZUZyKh9s3EdkaDC/uKYV9/ZuYUNYm2rB26GzBwHXuJPLVfV9L1Y7BfRT1TwRCQVWichHwK+Aaao6T0RmA+OB593v36tqaxG5C3gK5xRYYyqF7/YfZcbSVD7cuI8aocHc597cpq7d3MZUI96cfTQVuAJ4w531sIhcraqPl7WeqiqQ506Gul8K9ANGuvPnAlNwQmGw+xhgPjBLRMTdjjF+s3X/EWYkp/Lhxv1EhQUzqU8rft7L7nRmqidvjhRuBhJUtQBAROYCa4EyQ8FdNhiniag18CywHTisqvnuIhlAE/dxE2APgKrmi0guThPTwWLbnABMAGjWrJkX5RtzYTbvdcLgv9/uJzo8hAf7tWZ8rxbUrmFhYKovby9eqw0Unm1Uy9uNq+oZIEFEagMLgXbnV16J25wDzAFITEy0owhT7r7dm8uM5FQ+/vYAMeEhPNSvNfdYGJgA4U0o/C+wVkSW4ZyFdA0w+XxeRFUPu+tfhTMEd4h7tBAHZLqLZeJcE5EhIiE44ZNzPq9jzMXYlJlLUnIqizcfICYihIf7t+Geni2oVSPU36UZU2G8GfvoTRFZjtOvoMBvVXX/udZz7+X8gxsIkcB1OJ3Hy4ChOGcgjQXec1dZ5E5/4T6/1PoTTEXYmJFLUvI2lmzJomZECL8c0Ia7e7agVqSFgQk8ZV281hyn/T9XVfeJyFFgCNBGRGap6ulzbLsRMNftVwgC3lbV90VkMzBPRP6K0zfxorv8i8BrIpKG01R118X9aMaUbf2ewyQlp7J0axa1IkP51XWXMq5nPDUjLAxM4JLS/hkXka+A29x7MicAS3CakjrjHAH8vOLKLFliYqKmpKT4uwxTxazbc5ikJdtY9l02tWuE8vNeLRh7dTwxFgYmQIjIalVNLOm5spqPIlV1r/v4Z8BLqvp3EQkCbBwkU+Ws2f09SUtS+XRbNnVqhPLoDW0Ze3U80eHnM1iwMdVbWX8NRYe26Af8DkBVC0SKj3phTOW1etchpi9JZWXqQerUCOWxG9sy5ioLA2NKUtZfxVIReRvYB9QBlgKISCPgXP0JxvhdSvohkpKdMKgbFcbkm9oxukdzoiwMjClVWX8dv8QZZqIR0EtVf3DnXwL8j68LM+ZCfb3zEEnJ2/gsLYf60WE8fnM7ftajOTXCLAyMOZeyBsRTnNNGi89f69OKjLlAX+7IIWlJKl/scMLgf26+jFE9mlkYGHMe7K/FVHlfbM8hKXkbX+44RGxMOL+/5TJGXdmcyLBgf5dmTJVjoWCqJFXlix05TF+Sytc7D9EgJpw/DmzPyCubERFqYWDMhfJ26Oww4FJ38rsi/QvGVChV5fPtTjPR1+lOGDxxa3tGdLcwMKY8eDN0dh+cIa7TcU5TbSoiY1V1hW9LM+ZHqsqqtIMkLUklZdf3XFIzgj8N6sDwK5paGBhTjrw5Uvg7cL2qfgcgIpcCb+LcIc0Yn1JVVqYeZPqSbazZfZhGtSL4y+AO3JloYWCML3gTCqGFgQCgqtvcO6kZ4zOqyqfbsklKTmXt7sM0rhXBX4Z0ZFhiHOEhFgbG+Io3oZAiIi8Ar7vTowAbcMj4hKqy/Ltspiensn7PYZrUjuTJ2zoy9HILA2MqgjehMBG4H3jInV6Jcxc1Y8qNqrJ0axYzklNZn5FLk9qR/O/tnbijWxxhIUH+Ls+YgOFNKNynqs8AzxTOEJGHgSSfVWUCRkGB8snm/cxcmsa3e48QVyeSqbd34nYLA2P8wptQGMtPA2BcCfOM8Vr+mQI+2LiPWUvTSM3Ko0X9KP42tDNDujYhNNjCwBh/KesmOyOAkUALEVlU5KkYfrxfszHn5YczBSxck8lzy9NIzznOpQ2jSborgYGdGxMcZKPvGuNvZR0pfI4zQmp9nNNSCx0FNviyKFP9nPzhDO+szmD28u1kHj5BxyY1mf2zy7m+fUOCLAyMqTTKGhBvF7ALuKriyjHVzfHT+fzrq93MWbGDrKOn6NasNn+9rSN9Lo3F7sthTOVjYx8Znzh68gde+3IXL67cSc6x01zVsh7ThydwVat6FgbGVGIWCqZcHT5+mpc/S+flz3Zy5GQ+fdrG8kDf1iTG1/V3acYYL5xXKIhIHaCpqlqfgjnLwbxTvLByJ699kc6x02e4vn1DHuzXhk5xtfxdmjHmPHgzIN5yYJC77GogS0Q+U9Vf+bg2UwXszz3JnBU7+NfXuziVX8DAzo25v28r2l1S09+lGWMugDdHCrVU9YiI/Bx4VVWfEBE7Ughwew4dZ/an23knJYMzqgxJaMKkvq1oFRvt79KMMRfBm1AIEZFGwDDs3swBb+fBYzy3LI2FazMJEmFoYhwTr21F07o1/F2aMaYceBMKfwY+Bj5T1W9EpCWQ6tuyTGXz3f6jPLssjfc37CU0OIjRVzVnwjUtaVQr0t+lGWPK0TlDQVXfAd4pMr0DuMOXRZnKY1NmLjOXpvLxtweICgvm3mta8vNeLYmNCfd3acYYH/Cmo7klzjhHPQAFvgAeccPBVFOrd33PrKWpLPsum5iIEB7q34a7r46nTlSYv0szxviQN81H/8IZKvs2d/ounDuvXemroox/qCpf7Mhh1tI0Pt+eQ50aoTx6Q1tGX9WcmhF2XyVjAoE3oVBDVV8rMv26iDzqq4JMxSu8y9mspWmk7Pqe2Jhwfn/LZYy8shk1wuz6RmMCiTd/8R+JyGRgHk7z0XDgQxGpC6CqNmJqFfXDmQI+3LiPf67cwabMIzSuFcGfB3dgmN3/2JiA5U0oDHO//6LY/LtwQqJlSSuJSFPgVaChu9wcVU1yw+QtIB5IB4ap6vfiDIiTBNwMHAfGqeqa8/ppjFeOnvyBeV/v4eXPdrI39yQtY6PsxjbGGMC7s49aXOC284Ffq+oaEYkBVovIYpwb9CSr6lT3CGQy8FvgJqCN+3Ul8DzWb1GuMg+f4JXPdvLm13vIO5VPj5Z1+cuQjvRt28CGrzbGAN6dfVQD+BXQTFUniEgboK2qvl/Weqq6D+d+DKjqURHZAjQBBgN93MXmAstxQmEwzhXTCnwpIrVFpJG7HXMRNmbk8s+VO/hgo7Mrb+nUiHt7t7RxiYwxP+FN89HLOGMeXe1OZ+Jct1BmKBQlIvFAV+AroGGRD/r9OM1L4ATGniKrZbjzzgoFEZkATABo1qyZtyUEnIICZdl3Wfxz5Q6+3HGI6PAQ7ukZz7ieLWhS2y44M8aUzJtQaKWqw93bc6Kqx+U8Bhkb8SoAAA/ySURBVMQXkWhgAfBLdwwlz3OqqiKi51Owqs4B5gAkJiae17qB4OQPZ1i4NpMXVu5ge/YxGtWK4H9uvozh3ZvaaaXGmHPyJhROi0gkTmcxItIKOOXNxkUkFCcQ3lDVf7uzDxQ2C7ljKmW58zOBpkVWj3PnGS8cOnaa177YxWtfpnMw7zQdm9Qk6a4Ebu7UiNBg6zw2xnjHm1CYAvwXaCoibwA9gbvPtZJ7NPEisEVVnyny1CJgLDDV/f5ekfkPiMg8nA7mXOtPOLft2Xm8uGonC1ZncCq/gH7tGnBv75b0aFnX7nBmjDlv3px99ImIrMYZ5kKAh1X1oBfb7gmMBjaKyDp33uM4YfC2iIzHuQd04SmvH+KcjpqGc0rqOYMnUKkqX+88xD9X7iR56wFCg4O4o1sTxvdqQesGMf4uzxhThXlz9lGyqvYHPihhXqlUdRVOiJTkJ+u6Zx3df656Aln+mQI+2rSfF1buYH1GLnVqhPJgvzaM7tHcBqgzxpSLUkNBRCKAGkB99zachR/wNXHOCjIVJO9UPm99s4eXVu0k8/AJWtSP4q9DOnJHtzgiw+zKY2NM+SnrSOEXwC+BxjinpBaGwhFglo/rMsC+3BO88lk6//p6N0dP5tM9vi5TBnWgfzu72MwY4xulhoKqJgFJIvKgqs6swJoC3rd7c3lh5U7+s34vBarc7F5s1qVpbX+XZoyp5spqProC2FMYCCIyBufmOruAKTYQXvlSVZZvy+aFlTv4LC2HqLBgxlwVz9094+1Wl8aYClNW89E/gAEAInINzllDDwIJOBePDfV5dQHgVP4Z3lu7l3+u3EFqVh6X1Izgdze1467uzagVaRebGWMqVlmhEFzkaGA4ziinC4AFRU4xNedBVdl/5CSpB/JIy8ojNSuPxZsPcDDvFJc1qsm04V24pVNjG6nUGOM3ZYaCiISoaj7OKaQTvFwv4J0pUPYcOk5qlvPh73wdZXv2MfJO5XuWq10jlMub1eGeXi24ulU9u9jMGON3ZX24vwl8KiIHgRPASgARaQ3kVkBtld6p/DOkHzzu/td/1BMAOw4e43R+gWe5hjXDad0gmju6NaF1wxhax0bTpmE09aLCLAiMMZVKWWcfPSkiyUAj4BP34jKAIJy+hYBx7FQ+27N//K8/NSuP7Vl57Dp0nDMFzm4Rgbg6kbRpEMM1l8bSOjaa1g2jaRUbbX0Dxpgqo8xmIFX9soR523xXjn8dPn7a86GfVuQr8/AJzzIhQUJ8/SgubRjDLZ0b0bqB88HfKjbaLiQzxlR5Adk3cOjYabbsO/KTZp+Deac9y4SHBNEqNprE+DrcFduUNg2jad0gmub1omzUUWNMtRWQofDm17v528ffARATHkLrhtH0bdvA88HfOjaGJnUiCbarho0xASYgQ2Fg50YkNK1N6wbRNIgJt85eY4xxBWQoNK8XRfN6Uf4uwxhjKh1rHDfGGONhoWCMMcbDQsEYY4yHhYIxxhgPCwVjjDEeFgrGGGM8LBSMMcZ4WCgYY4zxsFAwxhjjYaFgjDHGw0LBGGOMh4WCMcYYDwsFY4wxHhYKxhhjPCwUjDHGeFgoGGOM8fBZKIjISyKSJSKbisyrKyKLRSTV/V7HnS8iMkNE0kRkg4h081VdxhhjSufLI4VXgBuLzZsMJKtqGyDZnQa4CWjjfk0AnvdhXcYYY0rhs1BQ1RXAoWKzBwNz3cdzgSFF5r+qji+B2iLSyFe1GWOMKVlF9yk0VNV97uP9QEP3cRNgT5HlMtx5PyEiE0QkRURSsrOzfVepMcYEIL91NKuqAnoB681R1URVTYyNjfVBZcYYE7gqOhQOFDYLud+z3PmZQNMiy8W584wxxlSgig6FRcBY9/FY4L0i88e4ZyH1AHKLNDMZY4ypICG+2rCIvAn0AeqLSAbwBDAVeFtExgO7gGHu4h8CNwNpwHHgbl/VZYwxpnQ+CwVVHVHKU/1LWFaB+31VizHGGO/YFc3GGGM8LBSMMcZ4WCgYY4zxsFAwxhjjYaFgjDHGw0LBGGOMh4WCMcYYDwsFY4wxHhYKxhhjPCwUjDHGeFgoGGOM8bBQMMYY42GhYIwxxsNCwRhjjIeFgjHGGA8LBWOMMR4WCsYYYzwsFIwxxnhYKBhjjPGwUDDGGONhoWCMMcbDQsEYY4yHhYIxxhgPCwVjjDEeFgrGGGM8LBSMMcZ4WCgYY4zxsFAwxhjjYaFgjDHGw0LBGGOMR4i/CyhKRG4EkoBg4AVVneqTF0pdDN++C0FBIMEQFFzke1Cx6TLmS1Ap2wj2fr4EgYhPfkxzESSojPeCnP/v3JgqotKEgogEA88C1wEZwDciskhVN5f7ix3eDTuWQcEZ0DNFvhcUmz4DaLm/vAlAXodIscCxfxhMaa59DDreUe6brTShAHQH0lR1B4CIzAMGA+UfCleMd768oVpCeJwBLfjxe6nPnSNw1F3WVC5K6b/XEn+35/k7L3N+seeMKU1EbZ9stjKFQhNgT5HpDODK4guJyARgAkCzZs18X5UIBIdQuXaVMcb4RpVr7FTVOaqaqKqJsbGx/i7HGGOqlcoUCplA0yLTce48Y4wxFaQyhcI3QBsRaSEiYcBdwCI/12SMMQGl0jSUq2q+iDwAfIxzSupLqvqtn8syxpiAUmlCAUBVPwQ+9HcdxhgTqCpT85Exxhg/s1AwxhjjYaFgjDHGQ1Sr7jAOIpIN7LrA1esDB8uxnKrO9sfZbH/8yPbF2arD/miuqiVe6FWlQ+FiiEiKqib6u47KwvbH2Wx//Mj2xdmq+/6w5iNjjDEeFgrGGGM8AjkU5vi7gErG9sfZbH/8yPbF2ar1/gjYPgVjjDE/FchHCsYYY4qxUDDGGOMREKEgIk1FZJmIbBaRb0XkYXd+XRFZLCKp7vc6/q61oohIsIisFZH33ekWIvKViKSJyFvuSLUBQURqi8h8EdkqIltE5KoAf2884v6dbBKRN0UkIpDeHyLykohkicimIvNKfD+IY4a7XzaISDf/VV4+AiIUgHzg16raHugB3C8i7YHJQLKqtgGS3elA8TCwpcj0U8A0VW0NfA94eb/SaiEJ+K+qtgO64OyXgHxviEgT4CEgUVU74oxYfBeB9f54Bbix2LzS3g83AW3crwnA8xVUo88ERCio6j5VXeM+PorzR98E5x7Qc93F5gJD/FNhxRKROOAW4AV3WoB+wHx3kUDaF7WAa4AXAVT1tKoeJkDfG64QIFJEQoAawD4C6P2hqiuAQ8Vml/Z+GAy8qo4vgdoi0qhiKvWNgAiFokQkHugKfAU0VNV97lP7gYZ+KquiTQceAwrc6XrAYVXNd6czcEIzELQAsoGX3ea0F0QkigB9b6hqJvA0sBsnDHKB1QTu+6NQae+Hku4tX6X3TUCFgohEAwuAX6rqkaLPqXNubrU/P1dEBgJZqrra37VUEiFAN+B5Ve0KHKNYU1GgvDcA3LbywThh2RiI4qdNKQGtur8fAiYURCQUJxDeUNV/u7MPFB7qud+z/FVfBeoJDBKRdGAeTrNAEs5hb+FNlwLp/tgZQIaqfuVOz8cJiUB8bwAMAHaqaraq/gD8G+c9E6jvj0KlvR+q3b3lAyIU3DbzF4EtqvpMkacWAWPdx2OB9yq6toqmqr9T1ThVjcfpQFyqqqOAZcBQd7GA2BcAqrof2CMibd1Z/YHNBOB7w7Ub6CEiNdy/m8L9EZDvjyJKez8sAsa4ZyH1AHKLNDNVSQFxRbOI9AJWAhv5sR39cZx+hbeBZjhDcA9T1eIdTNWWiPQBfqOqA0WkJc6RQ11gLfAzVT3lz/oqiogk4HS6hwE7gLtx/mEKyPeGiPwJGI5z1t5a4Oc47eQB8f4QkTeBPjhDZB8AngDepYT3gxucs3Ca2I4Dd6tqij/qLi8BEQrGGGO8ExDNR8YYY7xjoWCMMcbDQsEYY4yHhYIxxhgPCwVjjDEeFgqm0hCRMyKyrshXfDlv/xUR2elue42IXHWe6+eVsd2hJT13MUTkHhHZ6I6+uUlEBrvz/ywiA8ph+0NE5I/nWCZWRP57sa9lqo6Qcy9iTIU5oaoJJT3hng8uqlpQ0vPn4VFVnS8i1wP/ADoXe51gVT1zka9x0dxBC/8H6Kaque4QLbEAqlrmB/l5eAwYVNYCqpotIvtEpKeqflZOr2sqMTtSMJWWiMSLyHci8iqwCWgqIn9z/2veKCLD3eWCROQ5934Ii0XkQy/+c18BtHbXTxeRp0RkDXCniIxwt79JRJ4qVtM0914DySISW0LNl4vIpyKyWkQ+LjI0wnJ33RRx7tlwhYj82x2f/68l1NcAOArkAahqnqrudLf1iogMFZHEIkdVG0VE3edbich/3RpWiki7Euq8FDilqgeLbHOGiHwuIjuK7b93gVHn2J+mmrBQMJVJZJEPuYXuvDbAc6raAUgEEnDueTAA+Jv7oXs7EA+0B0YD3jQL3YpzhXuhHFXthhMWT+GMCZUAXCEihcMkRwEpbi2f4lzp6uGOrzUTGKqqlwMvAU8WWeS0qiYCs3GGSbgf6AiME5F6xepbj3M17U4ReVlEbi3+A6hqiqomuEdX/8UZ3RScG8s/6NbwG+C5En7+nsCaYvMaAb2AgcDUIvNTgN4lbMNUQ9Z8ZCqTs5qP3D6FXe449eB8YL3pNu8cEJFPgSvc+e+4TUv7RWRZGa/xNxH5Pc5w2UVvFPOW+/0KYLmqZrs1vIFzv4V3cYZIKVzudZzB4opqi/Mhv9hp7SIYZ/jpQovc7xuBbwvHyBGRHTiDquUULqiqZ0TkRree/sA0EblcVacU/4HcI6ZuwPVuM9PVwDtuDQDhJeyHRu4+KOpddx9uFpGiQ4Vn4YyYagKAhYKp7I6V8/YeVdX5Jcy/kNcpPkaM4HzYl3akUjhWUEGRx4XTP/lbdIdo/hr4WkQWAy8DU856QZGO7rxr3CAJwrn3QYl9M0WcAGqVUl/hz1Iowl3eBABrPjJVyUpguDj3l47F+Q/+a+Az4A63b6EhzmBmF+pr4FoRqS8iwcAInKYicP5eCtvaRwKriq37HRBbeFaTiISKSIcLKUJEGsvZ9/tNwBmIregytYE3gTGFRzbufUJ2isid7jIiIl1KeIktuH0qXrgUp0/HBAA7UjBVyUKc/oL1OP+lP6aq+0VkAT8O8bwHp60890JeQFX3ichknKGiBfhAVQuHST4GdHebn7JwRhItuu5pt4N2hji3+QzBucvdtxdQSijwtIg0Bk7iNPXcV2yZwUBz4J+FTUXuEcIo4Hm3zlCc0U3XF1t3BfB3ERE996iYfYEPLuBnMFWQjZJqqgURiVbVPLfD9mugp3uvBFMKEUkC/qOqS86x3ApgsKp+XzGVGX+yIwVTXbzvNqeEAX+xQPDK/wOuLGsBt5nuGQuEwGFHCsYYYzyso9kYY4yHhYIxxhgPCwVjjDEeFgrGGGM8LBSMMcZ4/H958TKMB/rmPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ns, vi_steps, label=\"Value Iteration\")\n",
    "plt.plot(ns, pi_steps, label=\"Policy Iteration\")\n",
    "\n",
    "plt.xlabel(\"Frog Problem Size (n)\")\n",
    "plt.ylabel(\"Steps to Solve\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Job-Hpping and Wages-Utility Maximization\n",
    "\n",
    "The state space of this MDP is $\\mathcal{S} = \\{s \\mid s = 1, 2, ..., n, n+1\\}$ where $1 \\leq s \\leq n$ are states for each of the $n$ jobs, and $s = n + 1$ is the unemployed state.\n",
    "\n",
    "The action space can be defined as $\\mathcal{A} = \\{c, r\\}$ where $c$ represents accepting a new job and $r$ represents rejecting a new job.\n",
    "\n",
    "The transition function $\\mathcal{P}(s, a, s')$ can be computed as $\\mathbb{P}\\left[S_{t+1} = s' \\mid S_t = s, A = a \\right]$.\n",
    "\n",
    "$$\\mathcal{P}(s, a, s) = 1 - \\alpha : \\forall s \\neq n + 1, a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{P}(s, a, s') = 0 : \\forall s \\neq s', s' \\neq n + 1, a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{P}(s, a, n + 1) = \\alpha : \\forall s \\neq n + 1, a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{P}(n + 1, c, s') = p_{s'} : \\forall s' \\neq n + 1$$\n",
    "$$\\mathcal{P}(n + 1, r, s')= 0.0 : \\forall s' \\neq n + 1$$\n",
    "$$\\mathcal{P}(n + 1, r, n + 1) = 1.0$$\n",
    "$$\\mathcal{P}(n + 1, c, n + 1)= 0.0$$\n",
    "\n",
    "The reward function $\\mathcal{R}(s, a)$ is:\n",
    "\n",
    "$$\\mathcal{R}(s, a) = \\log(w_s) : \\forall s \\neq n + 1, \\forall a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{R}(n + 1, c) = \\log \\left(\\mathbb{E}\\left[w_{s'} \\mid S_{t+1} = s'\\right]\\right) = \\log \\left(\\underset{1 \\leq i \\leq n}{\\sum}p_i w_i \\right)$$\n",
    "$$\\mathcal{R}(n + 1, r) = \\log(w_{0})$$\n",
    "\n",
    "Now we can write the general Bellman Optimality Equation:\n",
    "\n",
    "$$ \\pmb{V}^*(s) = \\underset{a \\in \\mathcal{A}}{\\max}\\left\\{ \\mathcal{R}(s, a) \n",
    "    + \\gamma \\cdot \\underset{s' \\in \\mathcal{S}}{\\sum} \\mathcal{P}(s, a, s') \\cdot \\pmb{V}^*(s') \\right\\}\n",
    "$$\n",
    "\n",
    "and solve for $\\pmb{V}^*$ with the Value Iteration algorithm. From there, we can back out $\\pi^*$, the optimal (deterministic) policy.\n",
    "\n",
    "$$\\pmb{Q}^*(s,a) = \\mathcal{R}(s,a) + \\gamma \\cdot \\underset{s' \\in \\mathcal{S}}{\\sum}\\mathcal{P}(s, a, s') \\cdot \\pmb{V}^*(s')$$\n",
    "$$\\pi^*(s) = \\underset{a \\in \\mathcal{A}}{\\arg \\max} \\left\\{ \\pmb{Q}^*(s,a) \\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import (\n",
    "    Mapping,\n",
    "    List, \n",
    "    Generic, \n",
    "    TypeVar, \n",
    "    Set\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar(\"S\")\n",
    "\n",
    "@dataclass\n",
    "class WageMaximizer(Generic[S]):\n",
    "    \"\"\"\n",
    "    Solves the Wage-Utility Maximization problem via the \n",
    "    Bellman Optimality Equation.\n",
    "    \"\"\"\n",
    "    gamma: float\n",
    "    alpha: float\n",
    "\n",
    "    employed_states: List[S]\n",
    "    unemployed_state: S\n",
    "    \n",
    "    employed_wages: np.array\n",
    "    unemployed_wage: float\n",
    "        \n",
    "    probs: np.array\n",
    "        \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Some more set up.\"\"\"\n",
    "        self._validate()\n",
    "        self.actions: Set[str] = {'c', 'r'}\n",
    "        self._state_probs = {\n",
    "            s: p for s, p in \n",
    "            zip(self.employed_states, self.probs)\n",
    "        }\n",
    "        self._state_wages = {\n",
    "            s: r for s, r in \n",
    "            zip(self.employed_states, self.employed_wages)\n",
    "        }\n",
    "        \n",
    "    def _validate(self) -> None:\n",
    "        \"\"\"Validate given problem parametersl.\"\"\"\n",
    "        a = len(self.employed_wages)\n",
    "        b = len(self.probs)\n",
    "        c = len(self.employed_states)\n",
    "        if not (a == b == c):\n",
    "            raise ValueError(\"Check parameter lengths.\")\n",
    "        if any(x <= 0 for x in self.employed_wages + [self.unemployed_wage]):\n",
    "            raise ValueError(\"Must have positive wages.\")\n",
    "        if self.probs.sum() != 1:\n",
    "            raise ValueError(\"Check transition probabilities.\")\n",
    "        if self.unemployed_state in self.employed_states:\n",
    "            raise ValueError(\n",
    "                f\"{self.unemployed_state} cannot also be an employed_state\"\n",
    "            )\n",
    "    \n",
    "    @property\n",
    "    def states(self) -> List[S]:\n",
    "        return self.employed_states + [self.unemployed_state]\n",
    "    \n",
    "    def P(self, state: S, action: str, next_state: S) -> float:\n",
    "        \"\"\"Return the (s, a, s') transition probability\"\"\"\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f\"{a=} is an invalid action\")\n",
    "        m: S = self.unemployed_state\n",
    "        if (state, action, next_state) == (m, 'c', m):\n",
    "            return 0\n",
    "        if (state, action, next_state) == (m, 'r', m):\n",
    "            return 1\n",
    "        if (state, action) == (m, 'r'):\n",
    "            return 0\n",
    "        if (state, action) == (m, 'c'):\n",
    "            return self._state_probs[next_state]\n",
    "        if next_state == m:\n",
    "            return self.alpha\n",
    "        if state == next_state:\n",
    "            return 1 - self.alpha\n",
    "        return 0\n",
    "    \n",
    "    def R(self, state: S, action: str) -> float:\n",
    "        \"\"\"Return the expected reward of (s, a).\"\"\"\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f\"{a=} is an invalid action\")\n",
    "        m: S = self.unemployed_state\n",
    "        if (state, action) == (m, 'r'):\n",
    "            return np.log(self.unemployed_wage)\n",
    "        if (state, action) == (m, 'c'):\n",
    "            return np.log(self.employed_wages @ self.probs)\n",
    "        return np.log(self._state_wages[state])\n",
    "    \n",
    "    def value_iteration(self) -> np.array:\n",
    "        \"\"\"Compute the optimal value function.\"\"\"\n",
    "        vk = np.zeros(len(self.states))\n",
    "        \n",
    "        def maximize(s: S, vfunc: np.array) -> float:\n",
    "            \"\"\"Maximize value function for a state over actions.\"\"\"\n",
    "            maximum = float(\"-inf\")\n",
    "            for a in self.actions:\n",
    "                _sum = sum(\n",
    "                    self.P(s, a, s_) * vfunc[j] \n",
    "                    for j, s_ in enumerate(self.states)\n",
    "                )\n",
    "                val = self.R(s, a) + self.gamma * _sum\n",
    "                maximum = max(val, maximum)\n",
    "            return maximum\n",
    "        \n",
    "        while True:\n",
    "            improvement = vk.copy()\n",
    "            for i, state in enumerate(self.states):\n",
    "                improvement[i] = maximize(state, vk)\n",
    "            if np.linalg.norm(improvement - vk) < 1e-5:\n",
    "                return {s: v for s, v in zip(solver.states, improvement)}\n",
    "            vk = improvement\n",
    "            \n",
    "    def find_optimal_policy(self) -> Mapping[S, str]:\n",
    "        \"\"\"Find the an optimal deterministic policy.\"\"\"\n",
    "        pi = {}\n",
    "        v_star = self.value_iteration()\n",
    "        \n",
    "        def q_star(s: S, a: str) -> float:\n",
    "            \"\"\"Compute Q^* for a (state, action) pair.\"\"\"\n",
    "            return (self.R(s, a) + self.gamma * \n",
    "                    sum(self.P(s, a, s_) * v_star[s_] \n",
    "                        for s_ in self.states))\n",
    "        \n",
    "        for s in self.states:\n",
    "            _max = float(\"-inf\")\n",
    "            action = None\n",
    "            for a in self.actions:\n",
    "                if q_star(s, a) > _max:\n",
    "                    _max = q_star(s, a)\n",
    "                    action = a\n",
    "            pi[s] = action\n",
    "        return pi\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.7760895874094604,\n",
       " 2: 0.014389425423822473,\n",
       " 3: 1.5377897493950983,\n",
       " 4: 2.1527521243062777,\n",
       " 5: 1.221655618964289,\n",
       " 6: 1.3094432712224158}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma: float = 0.1\n",
    "alpha: float = 0.1\n",
    "\n",
    "employed_states: List[int] = [1, 2, 3, 4, 5]\n",
    "unemployed_state: int = 6\n",
    "\n",
    "employed_wages: List[float] = [2, 1, 4, 7, 3]\n",
    "unemployed_wage: float = 1\n",
    "    \n",
    "probs: List[float] = [0.1, 0.2, 0.4, 0.1, 0.2]\n",
    "    \n",
    "solver = WageMaximizer[int](\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    employed_states=employed_states,\n",
    "    unemployed_state=unemployed_state,\n",
    "    employed_wages=np.array(employed_wages),\n",
    "    unemployed_wage=unemployed_wage,\n",
    "    probs=np.array(probs)\n",
    ")\n",
    "v_star = solver.value_iteration()\n",
    "v_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'r', 2: 'r', 3: 'r', 4: 'r', 5: 'r', 6: 'c'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that for all the 'employed' states, it doesn't matter what our\n",
    "# policy is because our action does not affect the transition\n",
    "# probabilities\n",
    "opt_policy = solver.find_optimal_policy()\n",
    "opt_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Two-Stores Inventory Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The states of this problem can be represented as four-tuples, $\\langle\\alpha_0, \\beta_0, \\alpha_1, \\beta_1\\rangle$. The actions are three-tuples $\\langle\\theta_0, \\delta, \\theta_1\\rangle$, where $\\theta_i$ is how much inventory store $i$ ordered from the supplier and $\\delta$ is how much inventory will be shifted from store $0$ to store $1$. Note that $\\delta$ can be negative.\n",
    "\n",
    "From this, we can see that the state transitions $s \\to s'$ take the form \n",
    "\n",
    "$$\\langle\\alpha_0, \\beta_0, \\alpha_1, \\beta_1\\rangle \\to \\langle\\max\\left\\{\\alpha_0 + \\beta_0 - \\delta - X_0, 0\\right\\}, \\theta_0, \\max\\left\\{\\alpha_1 + \\beta_1 + \\delta - X_1, 0\\right\\}, \\theta_1\\rangle$$\n",
    "\n",
    "for a given action tuple and where\n",
    "\n",
    "$$\\mathbb{P}[X_i = x_i] =  \\frac{{e^{ - \\lambda_0 } \\lambda_0^{x_i} }}{{x_i!}}$$\n",
    "\n",
    "We will define the rewards of our two store MDP as the sum of the rewards of the individual one store MDPs.\n",
    "\n",
    "$$\n",
    "\\mathcal{R}\\left( \\langle\\alpha_0, \\beta_0, \\alpha_1, \\beta_1\\rangle, a\\right) = \\mathcal{R}_0\\left(\\langle\\alpha_0, \\beta_0\\rangle, a\\right) + \\mathcal{R}_1\\left(\\langle\\alpha_1, \\beta_1\\rangle, a\\right)\n",
    "$$\n",
    "\n",
    "<!-- Since store $0$ and store $1$ face independent demands $x_0$, $x_1$ on any given day, we can compute the joint probability of demands as \n",
    "\n",
    "$$\n",
    "    \\mathbb{P}[X_0 = x_0, X_1 = x_1] = \\mathbb{P}[X_0 = x_0] \\cdot \\mathbb{P}[X_1 = x_1] \n",
    "$$\n",
    "\n",
    "This leads us to the observation\n",
    "\n",
    "$$\\mathcal{P}\\left( \\langle\\alpha_0, \\beta_0, \\alpha_1, \\beta_1\\rangle, a, \\langle\\alpha_0', \\beta_0', \\alpha_1', \\beta_1'\\rangle \\right) = \\mathcal{P}_0\\left(\\langle\\alpha_0, \\beta_0\\rangle, a, \\langle\\alpha_0', \\beta_0'\\rangle\\right) \\cdot \\mathcal{P}_1\\left(\\langle\\alpha_1, \\beta_1\\rangle, a, \\langle\\alpha_1', \\beta_1'\\rangle\\right)$$\n",
    "\n",
    "where $\\mathcal{P}_i(s_i, a, s_i')$ represent the dynamics of the single store MDP.  -->\n",
    "\n",
    "We will now define $\\mathcal{P}_i$ where $\\mathcal{P}_i(s_i, a, s_i')$ represent the dynamics of a single store MDP. We can then directly compute the rewards at each $s'$. Every morning, the opening inventory in store $i$ is $\\alpha_i + \\beta_i - \\delta\\pmb{I}$ where $\\pmb{I}$ is an indicator variable (1 for $i=0$, -1 for $i=1$).\n",
    "\n",
    "If $\\alpha_i + \\beta_i - \\delta\\pmb{I} >= x_i$:\n",
    "- $\\mathcal{P}_i(s, a, s') = \\mathbb{P}[X_i = x_i]$ \n",
    "- $R_i(s') = -h_i\\alpha_i$\n",
    "\n",
    "If $\\alpha_i + \\beta_i - \\delta\\pmb{I} < x_i$:\n",
    "- $\\mathcal{P}_i(s, a, s'=0) = \\mathbb{P}[X_i > \\alpha_i + \\beta_i - \\delta\\pmb{I}]$ \n",
    "<!-- - $\\mathcal{R}_i(s'=0) = -h_i\\alpha_i - p_i (x_i - \\alpha_i + \\beta_i - \\delta\\pmb{I})$ -->\n",
    "- $\\mathcal{R}_i(s'=0) = -h_i\\alpha_i - p_i \\sum_{j = \\alpha_i + \\beta_i - \\delta\\pmb{I} + 1}^\\infty f(j) \\cdot(j - (\\alpha_i + \\beta_i - \\delta\\pmb{I}))$\n",
    "\n",
    "where $h$ and $p$ are holding and stockout costs, respectively. From these, we can express\n",
    "\n",
    "$$\\mathcal{R}_i(s, a) = K_1\\theta_i + \\frac{K_2}{2}\\delta + \\sum_{s'} \\mathcal{P}_i(s, a, s') \\cdot R_i(s')$$\n",
    "\n",
    "and compute $\\mathcal{R}(s, a) = \\mathcal{R}_0(s, a) + \\mathcal{R}_1(s, a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Iterable, Dict\n",
    "import itertools as it\n",
    "\n",
    "from scipy.stats import poisson\n",
    "\n",
    "from rl.distribution import Categorical\n",
    "from rl.markov_decision_process import (\n",
    "    FiniteMarkovDecisionProcess as fMDP,\n",
    "    StateActionMapping\n",
    ")\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = Tuple[InventoryState, InventoryState]\n",
    "Action = Tuple[int, int, int]\n",
    "\n",
    "@dataclass\n",
    "class TwoStoreInventory(fMDP[State, Action]):\n",
    "    \"\"\"Representation of the Two-Store Inventory Control problem.\"\"\"\n",
    "    \n",
    "    capacities: Tuple[int, int]\n",
    "    lambdas: Tuple[float, float]\n",
    "    holding_costs: Tuple[float, float]\n",
    "    stockout_costs: Tuple[float, float]\n",
    "    supplier_cost: float\n",
    "    transfer_cost: float\n",
    "        \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Instantiate base class with proper distributions.\"\"\"\n",
    "        super().__init__(mapping=self._map_transitions())\n",
    "        \n",
    "    def _get_probability_reward(\n",
    "        self, \n",
    "        s: State, \n",
    "        a: Action, \n",
    "        x: int\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"Compute joint probability of this transition.\"\"\"\n",
    "        delta: int = a[1]\n",
    "            \n",
    "        def _prob_reward(i: int) -> float:\n",
    "            \"\"\"Compute probability of single store MDP transition.\"\"\"\n",
    "            _map = {0: 1, 1: -1}\n",
    "            indicator = _map[i]\n",
    "            Poiss = poisson(self.lambdas[i])\n",
    "            ip = s[i].inventory_position()\n",
    "            if ip - delta * indicator >= x[i]:\n",
    "                p = Poiss.pmf(x[i])\n",
    "                r = -self.h[i] * s[i].on_hand\n",
    "                return p, r\n",
    "            p = Poiss.cdf(x[i])\n",
    "            r = -self.h[i] * s[i].on_hand\n",
    "            r -= self.p[i] * self.lambdas[i] * (1 - Poiss(ip - 1))\n",
    "            r -= ip * (1 - Poiss(ip))\n",
    "            return p, r\n",
    "\n",
    "        P, R = _prob_reward(0)\n",
    "        P_, R_ = _prob_reward(1)\n",
    "        return P * P_, R + R_\n",
    "      \n",
    "    \n",
    "    def _orders_filled(self, _state: State, _action: Action) -> Tuple[int, int]:\n",
    "        \"\"\"We can create next states from enumeration of filled orders.\"\"\"\n",
    "        delta: int = _action[1]\n",
    "\n",
    "        def _orders(i: int) -> int:\n",
    "            \"\"\"Possible orders for single store.\"\"\"\n",
    "            _map = {0: 1, 1: -1}\n",
    "            indicator = _map[i]\n",
    "            ip: int = _state[i].inventory_position()\n",
    "            total_sellable: int = ip - delta * indicator\n",
    "            for j in range(total_sellable + 1):\n",
    "                yield j\n",
    "\n",
    "        for a, b in it.product(_orders(0), _orders(1)):\n",
    "            yield a, b \n",
    "                \n",
    "    def _map_transitions(self) -> StateActionMapping[State, Action]:\n",
    "        \"\"\"Construct the transition probability table.\"\"\"\n",
    "        StateRewardPair = Tuple[State, float]\n",
    "        mapping: Dict[State, Dict[Action, Categorical[StateRewardPair]]] = {}\n",
    "                    \n",
    "        for state in self.all_states:\n",
    "            action_to_dist: Dict[Action, Categorical[StateRewardPair]] = {}\n",
    "            for action in self.available_actions(state):\n",
    "                state_reward_to_prob: Dict[StateRewardPair, float] = {}\n",
    "                for demand in self._orders_filled(state, action):\n",
    "                    P, R = self._get_probability_reward(state, action, demand)\n",
    "                    state_ = self._construct_next_state(state, action, demand)\n",
    "                    state_reward_to_prob[(state_, R)] = P\n",
    "                action_to_dist[action] = Categorical(state_reward_to_prob)\n",
    "            mapping[state] = action_to_dist\n",
    "        return mapping\n",
    "    \n",
    "    def _construct_next_state(\n",
    "        self,\n",
    "        _state: State, \n",
    "        _action: Action, \n",
    "        _demand: Tuple[int, int]\n",
    "    ) -> State:\n",
    "        \"\"\"Form the next state.\"\"\"\n",
    "        delta: int = _action[1]\n",
    "        x = InventoryState(\n",
    "            _state[0].inventory_position() - delta - _demand[0],\n",
    "            _action[0]\n",
    "        )\n",
    "        y = InventoryState(\n",
    "            _state[1].inventory_position() + delta - _demand[1],\n",
    "            _action[2]\n",
    "        )\n",
    "        return x, y\n",
    "                \n",
    "    @property\n",
    "    def cap(self) -> Tuple[int, int]:\n",
    "        \"\"\"Short alias.\"\"\"\n",
    "        return self.capacities\n",
    "    \n",
    "    @property\n",
    "    def h(self) -> Tuple[float, float]:\n",
    "        \"\"\"Short alias.\"\"\"\n",
    "        return self.holding_costs\n",
    "    \n",
    "    @property\n",
    "    def p(self) -> Tuple[float, float]:\n",
    "        \"\"\"Short alias.\"\"\"\n",
    "        return self.stockout_costs\n",
    "\n",
    "    @property\n",
    "    def all_states(self) -> Iterable[State]:\n",
    "        \"\"\"Yields all states of this MDP.\"\"\"\n",
    "        \n",
    "        def _states(i: int) -> Iterable[InventoryState]:\n",
    "            \"\"\"Generate all states given a single store capacity.\"\"\"\n",
    "            for alpha in range(self.cap[i] + 1):\n",
    "                for beta in range(self.cap[i] + 1 - alpha):\n",
    "                    yield InventoryState(alpha, beta)\n",
    "                    \n",
    "        for a, b in it.product(_states(0), _states(1)):\n",
    "            yield a, b\n",
    "            \n",
    "    def available_actions(self, s: State) -> Iterable[Action]:\n",
    "        \"\"\"Given a current state, yield possible next actions.\"\"\"\n",
    "        ips = s[0].inventory_position(), s[1].inventory_position()\n",
    "        n_moveable_up = min(s[0].on_hand, self.cap[1] - ips[1])\n",
    "        n_moveable_down = min(s[1].on_hand, self.cap[0] - ips[0])\n",
    "        \n",
    "        for transfer in range(-n_moveable_down, n_moveable_up + 1):\n",
    "            for a in range(self.cap[0] - ips[0] + transfer + 1):\n",
    "                for b in range(self.cap[1] - ips[1] - transfer + 1):\n",
    "                    msg = f\"{s}, {transfer=}\"\n",
    "                    assert transfer <= s[0].on_hand, msg\n",
    "                    assert ips[0] - transfer <= self.cap[0], msg\n",
    "                    assert -transfer <= s[1].on_hand, msg\n",
    "                    assert ips[1] + transfer <= self.cap[1], msg\n",
    "                    yield a, transfer, b\n",
    "                \n",
    "                      \n",
    "    def possible_next_states(s: State, a: Action) -> Iterable[State]:\n",
    "        \"\"\"Yield all possible next states for a given s, a pair.\"\"\"\n",
    "        \n",
    "        def _single_next_states(on_hand: int, ordered: int) -> InventoryState:\n",
    "            \"\"\"Account for all possible daily demands.\"\"\"\n",
    "            for demand in range(on_hand + 1):\n",
    "                yield InventoryState(on_hand - demand, ordered)\n",
    "        \n",
    "        order0, transfer, order1 = a\n",
    "        morning_inventory0 = s[0].inventory_position() - transfer\n",
    "        morning_inventory1 = s[1].inventory_position() + transfer\n",
    "        for a, b in it.product(\n",
    "            _next_states(morning_inventory0, order0),\n",
    "            _next_states(morning_inventory1, order1)\n",
    "        ):\n",
    "            yield a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = TwoStoreInventory(\n",
    "    capacities=(3, 2),\n",
    "    lambdas=(1, 3),\n",
    "    holding_costs=(2, 3),\n",
    "    stockout_costs=(7, 6),\n",
    "    supplier_cost=8,\n",
    "    transfer_cost=2,\n",
    ")\n",
    "opt_vf, opt_policy = value_iteration_result(mdp, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=0)): 0.0,\n",
       " (InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=1)): -0.8709674523163493,\n",
       " (InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=2)): -2.0857377676002486,\n",
       " (InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=0)): -3.870967452316349,\n",
       " (InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=1)): -5.085737767600248,\n",
       " (InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=2, on_order=0)): -8.08573776760025,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=0)): -1.2919349046326987,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=1)): -2.2015801042334386,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=2)): -3.4588611763192936,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=0)): -5.201580104233439,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=1)): -6.458861176319293,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=2, on_order=0)): -9.458861176319294,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=0)): -3.224412562428527,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=1)): -4.1715160760366405,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=2)): -5.494957319406379,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=1, on_order=0)): -7.171516076036641,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=1, on_order=1)): -8.49495731940638,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=2, on_order=0)): -11.49495731940638,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=0, on_order=0)): -5.730774953307169,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=0, on_order=1)): -6.7569231260883305,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=0, on_order=2)): -8.203691392297035,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=1, on_order=0)): -9.756923126088331,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=1, on_order=1)): -11.203691392297033,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=2, on_order=0)): -14.203691392297035,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=0)): -2.870967452316349,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=1)): -4.085737767600249,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=2)): -5.458861176319293,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=0)): -7.085737767600248,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=1)): -8.458861176319294,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=2, on_order=0)): -11.458861176319292,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=0)): -4.201580104233439,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=1)): -5.458861176319293,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=2)): -7.494957319406379,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=0)): -8.458861176319294,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=1)): -10.494957319406378,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=2, on_order=0)): -13.494957319406382,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=0)): -6.17151607603664,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=1)): -7.494957319406379,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=2)): -10.203691392297033,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=1, on_order=0)): -10.494957319406378,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=1, on_order=1)): -13.203691392297037,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=2, on_order=0)): -16.20369139229703,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=0)): -6.085737767600248,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=1)): -7.458861176319292,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=2)): -9.49495731940638,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=0)): -10.458861176319294,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=1)): -12.494957319406378,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=2, on_order=0)): -15.494957319406378,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=0)): -7.458861176319292,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=1)): -9.49495731940638,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=2)): -12.203691392297033,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=0)): -12.494957319406378,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=1)): -15.203691392297031,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=2, on_order=0)): -18.20369139229703,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=0)): -9.458861176319294,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=1)): -11.49495731940638,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=2)): -14.203691392297035,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=0)): -14.494957319406382,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=1)): -17.20369139229703,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=2, on_order=0)): -20.203691392297035}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 2, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 2, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 2, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
