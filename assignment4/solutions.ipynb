{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241: Assigment 4\n",
    "\n",
    "## Question 3\n",
    "\n",
    "The state space of this MDP is $\\mathcal{S} = \\{s \\mid s = 1, 2, ..., n, n+1\\}$ where $1 \\leq s \\leq n$ are states for each of the $n$ jobs, and $s = n + 1$ is the unemployed state.\n",
    "\n",
    "The action space can be defined as $\\mathcal{A} = \\{c, r\\}$ where $c$ represents accepting a new job and $r$ represents rejecting a new job.\n",
    "\n",
    "The transition function $\\mathcal{P}(s, a, s')$ can be computed as $\\mathbb{P}\\left[S_{t+1} = s' \\mid S_t = s, A = a \\right]$.\n",
    "\n",
    "$$\\mathcal{P}(s, a, s) = 1 - \\alpha : \\forall s \\neq n + 1, a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{P}(s, a, s') = 0 : \\forall s \\neq s', s' \\neq n + 1, a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{P}(s, a, n + 1) = \\alpha : \\forall s \\neq n + 1, a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{P}(n + 1, c, s') = p_{s'} : \\forall s' \\neq n + 1$$\n",
    "$$\\mathcal{P}(n + 1, r, s')= 0.0 : \\forall s' \\neq n + 1$$\n",
    "$$\\mathcal{P}(n + 1, r, n + 1) = 1.0$$\n",
    "$$\\mathcal{P}(n + 1, c, n + 1)= 0.0$$\n",
    "\n",
    "The reward function $\\mathcal{R}(s, a)$ is:\n",
    "\n",
    "$$\\mathcal{R}(s, a) = \\log(w_s) : \\forall s \\neq n + 1, \\forall a \\in \\mathcal{A}$$\n",
    "$$\\mathcal{R}(n + 1, c) = \\log \\left(\\mathbb{E}\\left[w_{s'} \\mid S_{t+1} = s'\\right]\\right) = \\log \\left(\\underset{1 \\leq i \\leq n}{\\sum}p_i w_i \\right)$$\n",
    "$$\\mathcal{R}(n + 1, r) = \\log(w_{0})$$\n",
    "\n",
    "Now we can write the general Bellman Optimality Equation:\n",
    "\n",
    "$$ \\pmb{V}^*(s) = \\underset{a \\in \\mathcal{A}}{\\max}\\left\\{ \\mathcal{R}(s, a) \n",
    "    + \\gamma \\cdot \\underset{s' \\in \\mathcal{S}}{\\sum} \\mathcal{P}(s, a, s') \\cdot \\pmb{V}^*(s') \\right\\}\n",
    "$$\n",
    "\n",
    "and solve for $\\pmb{V}^*$ with the Value Iteration algorithm. From there, we can back out $\\pi^*$, the optimal (deterministic) policy.\n",
    "\n",
    "$$\\pmb{Q}^*(s,a) = \\mathcal{R}(s,a) + \\gamma \\cdot \\underset{s' \\in \\mathcal{S}}{\\sum}\\mathcal{P}(s, a, s') \\cdot \\pmb{V}^*(s')$$\n",
    "$$\\pi^*(s) = \\underset{a \\in \\mathcal{A}}{\\arg \\max} \\left\\{ \\pmb{Q}^*(s,a) \\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import (\n",
    "    Mapping,\n",
    "    List, \n",
    "    Generic, \n",
    "    TypeVar, \n",
    "    Set\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar(\"S\")\n",
    "\n",
    "@dataclass\n",
    "class WageMaximizer(Generic[S]):\n",
    "    \"\"\"\n",
    "    Solves the Wage-Utility Maximization problem via the \n",
    "    Bellman Optimality Equation.\n",
    "    \"\"\"\n",
    "    gamma: float\n",
    "    alpha: float\n",
    "\n",
    "    employed_states: List[S]\n",
    "    unemployed_state: S\n",
    "    \n",
    "    employed_wages: np.array\n",
    "    unemployed_wage: float\n",
    "        \n",
    "    probs: np.array\n",
    "        \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Some more set up.\"\"\"\n",
    "        self._validate()\n",
    "        self.actions: Set[str] = {'c', 'r'}\n",
    "        self._state_probs = {\n",
    "            s: p for s, p in \n",
    "            zip(self.employed_states, self.probs)\n",
    "        }\n",
    "        self._state_wages = {\n",
    "            s: r for s, r in \n",
    "            zip(self.employed_states, self.employed_wages)\n",
    "        }\n",
    "        \n",
    "    def _validate(self) -> None:\n",
    "        \"\"\"Validate given problem parametersl.\"\"\"\n",
    "        a = len(self.employed_wages)\n",
    "        b = len(self.probs)\n",
    "        c = len(self.employed_states)\n",
    "        if not (a == b == c):\n",
    "            raise ValueError(\"Check parameter lengths.\")\n",
    "        if any(x <= 0 for x in self.employed_wages + [self.unemployed_wage]):\n",
    "            raise ValueError(\"Must have positive wages.\")\n",
    "        if self.probs.sum() != 1:\n",
    "            raise ValueError(\"Check transition probabilities.\")\n",
    "        if self.unemployed_state in self.employed_states:\n",
    "            raise ValueError(\n",
    "                f\"{self.unemployed_state} cannot also be an employed_state\"\n",
    "            )\n",
    "    \n",
    "    @property\n",
    "    def states(self) -> List[S]:\n",
    "        return self.employed_states + [self.unemployed_state]\n",
    "    \n",
    "    def P(self, state: S, action: str, next_state: S) -> float:\n",
    "        \"\"\"Return the (s, a, s') transition probability\"\"\"\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f\"{a=} is an invalid action\")\n",
    "        m: S = self.unemployed_state\n",
    "        if (state, action, next_state) == (m, 'c', m):\n",
    "            return 0\n",
    "        if (state, action, next_state) == (m, 'r', m):\n",
    "            return 1\n",
    "        if (state, action) == (m, 'r'):\n",
    "            return 0\n",
    "        if (state, action) == (m, 'c'):\n",
    "            return self._state_probs[next_state]\n",
    "        if next_state == m:\n",
    "            return self.alpha\n",
    "        if state == next_state:\n",
    "            return 1 - self.alpha\n",
    "        return 0\n",
    "    \n",
    "    def R(self, state: S, action: str) -> float:\n",
    "        \"\"\"Return the expected reward of (s, a).\"\"\"\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f\"{a=} is an invalid action\")\n",
    "        m: S = self.unemployed_state\n",
    "        if (state, action) == (m, 'r'):\n",
    "            return np.log(self.unemployed_wage)\n",
    "        if (state, action) == (m, 'c'):\n",
    "            return np.log(self.employed_wages @ self.probs)\n",
    "        return np.log(self._state_wages[state])\n",
    "    \n",
    "    def value_iteration(self) -> np.array:\n",
    "        \"\"\"Compute the optimal value function.\"\"\"\n",
    "        vk = np.zeros(len(self.states))\n",
    "        \n",
    "        def maximize(s: S, vfunc: np.array) -> float:\n",
    "            \"\"\"Maximize value function for a state over actions.\"\"\"\n",
    "            maximum = float(\"-inf\")\n",
    "            for a in self.actions:\n",
    "                _sum = sum(\n",
    "                    self.P(s, a, s_) * vfunc[j] \n",
    "                    for j, s_ in enumerate(self.states)\n",
    "                )\n",
    "                val = self.R(s, a) + self.gamma * _sum\n",
    "                maximum = max(val, maximum)\n",
    "            return maximum\n",
    "        \n",
    "        while True:\n",
    "            improvement = vk.copy()\n",
    "            for i, state in enumerate(self.states):\n",
    "                improvement[i] = maximize(state, vk)\n",
    "            if np.linalg.norm(improvement - vk) < 1e-5:\n",
    "                return {s: v for s, v in zip(solver.states, improvement)}\n",
    "            vk = improvement\n",
    "            \n",
    "    def find_optimal_policy(self) -> Mapping[S, str]:\n",
    "        \"\"\"Find the an optimal deterministic policy.\"\"\"\n",
    "        pi = {}\n",
    "        v_star = self.value_iteration()\n",
    "        \n",
    "        def q_star(s: S, a: str) -> float:\n",
    "            \"\"\"Compute Q^* for a (state, action) pair.\"\"\"\n",
    "            return (self.R(s, a) + self.gamma * \n",
    "                    sum(self.P(s, a, s_) * v_star[s_] \n",
    "                        for s_ in self.states))\n",
    "        \n",
    "        for s in self.states:\n",
    "            _max = float(\"-inf\")\n",
    "            action = None\n",
    "            for a in self.actions:\n",
    "                if q_star(s, a) > _max:\n",
    "                    _max = q_star(s, a)\n",
    "                    action = a\n",
    "            pi[s] = action\n",
    "        return pi\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.7760895874094604,\n",
       " 2: 0.014389425423822473,\n",
       " 3: 1.5377897493950983,\n",
       " 4: 2.1527521243062777,\n",
       " 5: 1.221655618964289,\n",
       " 6: 1.3094432712224158}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma: float = 0.1\n",
    "alpha: float = 0.1\n",
    "\n",
    "employed_states: List[int] = [1, 2, 3, 4, 5]\n",
    "unemployed_state: int = 6\n",
    "\n",
    "employed_wages: List[float] = [2, 1, 4, 7, 3]\n",
    "unemployed_wage: float = 1\n",
    "    \n",
    "probs: List[float] = [0.1, 0.2, 0.4, 0.1, 0.2]\n",
    "    \n",
    "solver = WageMaximizer[int](\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    employed_states=employed_states,\n",
    "    unemployed_state=unemployed_state,\n",
    "    employed_wages=np.array(employed_wages),\n",
    "    unemployed_wage=unemployed_wage,\n",
    "    probs=np.array(probs)\n",
    ")\n",
    "v_star = solver.value_iteration()\n",
    "v_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'r', 2: 'r', 3: 'r', 4: 'r', 5: 'r', 6: 'c'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that for all the 'employed' states, it doesn't matter what our\n",
    "# policy is because our action does not affect the transition\n",
    "# probabilities\n",
    "opt_policy = solver.find_optimal_policy()\n",
    "opt_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The states of this problem can be represented as four-tuples, $\\langle\\alpha_0, \\beta_0, \\alpha_1, \\beta_1\\rangle$. The actions are three-tuples $\\langle\\theta_0, \\delta, \\theta_1\\rangle$, where $\\theta_i$ is how much inventory store $i$ ordered from the supplier and $\\delta$ is how much inventory will be shifted from store $0$ to store $1$. Note that $\\delta$ can be negative.\n",
    "\n",
    "From this, we can see that the state transitions $s \\to s'$ take the form \n",
    "\n",
    "$$\\langle\\alpha_0, \\beta_0, \\alpha_1, \\beta_1\\rangle \\to \\langle\\max\\left\\{\\alpha_0 + \\beta_0 - \\delta - X_0, 0\\right\\}, \\theta_0, \\max\\left\\{\\alpha_1 + \\beta_1 + \\delta - X_1, 0\\right\\}, \\theta_1\\rangle$$\n",
    "\n",
    "for a given action tuple and where\n",
    "\n",
    "$$\\mathbb{P}[X_i = x_i] =  \\frac{{e^{ - \\lambda_0 } \\lambda_0^{x_i} }}{{x_i!}}$$\n",
    "\n",
    "We will define the rewards of our two store MDP as the sum of the rewards of the individual one store MDPs.\n",
    "\n",
    "$$\n",
    "\\mathcal{R}\\left( \\langle\\alpha_0, \\beta_0, \\alpha_1, \\beta_1\\rangle, a\\right) = \\mathcal{R}_0\\left(\\langle\\alpha_0, \\beta_0\\rangle, a\\right) + \\mathcal{R}_1\\left(\\langle\\alpha_1, \\beta_1\\rangle, a\\right)\n",
    "$$\n",
    "\n",
    "<!-- Since store $0$ and store $1$ face independent demands $x_0$, $x_1$ on any given day, we can compute the joint probability of demands as \n",
    "\n",
    "$$\n",
    "    \\mathbb{P}[X_0 = x_0, X_1 = x_1] = \\mathbb{P}[X_0 = x_0] \\cdot \\mathbb{P}[X_1 = x_1] \n",
    "$$\n",
    "\n",
    "This leads us to the observation\n",
    "\n",
    "$$\\mathcal{P}\\left( \\langle\\alpha_0, \\beta_0, \\alpha_1, \\beta_1\\rangle, a, \\langle\\alpha_0', \\beta_0', \\alpha_1', \\beta_1'\\rangle \\right) = \\mathcal{P}_0\\left(\\langle\\alpha_0, \\beta_0\\rangle, a, \\langle\\alpha_0', \\beta_0'\\rangle\\right) \\cdot \\mathcal{P}_1\\left(\\langle\\alpha_1, \\beta_1\\rangle, a, \\langle\\alpha_1', \\beta_1'\\rangle\\right)$$\n",
    "\n",
    "where $\\mathcal{P}_i(s_i, a, s_i')$ represent the dynamics of the single store MDP.  -->\n",
    "\n",
    "We will now define $\\mathcal{P}_i$ where $\\mathcal{P}_i(s_i, a, s_i')$ represent the dynamics of a single store MDP. We can then directly compute the rewards at each $s'$. Every morning, the opening inventory in store $i$ is $\\alpha_i + \\beta_i - \\delta\\pmb{I}$ where $\\pmb{I}$ is an indicator variable (1 for $i=0$, -1 for $i=1$).\n",
    "\n",
    "If $\\alpha_i + \\beta_i - \\delta\\pmb{I} >= x_i$:\n",
    "- $\\mathcal{P}_i(s, a, s') = \\mathbb{P}[X_i = x_i]$ \n",
    "- $R_i(s') = -h_i\\alpha_i$\n",
    "\n",
    "If $\\alpha_i + \\beta_i - \\delta\\pmb{I} < x_i$:\n",
    "- $\\mathcal{P}_i(s, a, s'=0) = \\mathbb{P}[X_i > \\alpha_i + \\beta_i - \\delta\\pmb{I}]$ \n",
    "<!-- - $\\mathcal{R}_i(s'=0) = -h_i\\alpha_i - p_i (x_i - \\alpha_i + \\beta_i - \\delta\\pmb{I})$ -->\n",
    "- $\\mathcal{R}_i(s'=0) = -h_i\\alpha_i - p_i \\sum_{j = \\alpha_i + \\beta_i - \\delta\\pmb{I} + 1}^\\infty f(j) \\cdot(j - (\\alpha_i + \\beta_i - \\delta\\pmb{I}))$\n",
    "\n",
    "where $h$ and $p$ are holding and stockout costs, respectively. From these, we can express\n",
    "\n",
    "$$\\mathcal{R}_i(s, a) = K_1\\theta_i + \\frac{K_2}{2}\\delta + \\sum_{s'} \\mathcal{P}_i(s, a, s') \\cdot R_i(s')$$\n",
    "\n",
    "and compute $\\mathcal{R}(s, a) = \\mathcal{R}_0(s, a) + \\mathcal{R}_1(s, a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Iterable, Dict\n",
    "import itertools as it\n",
    "\n",
    "from scipy.stats import poisson\n",
    "\n",
    "from rl.distribution import Categorical\n",
    "from rl.markov_decision_process import (\n",
    "    FiniteMarkovDecisionProcess as fMDP,\n",
    "    StateActionMapping\n",
    ")\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = Tuple[InventoryState, InventoryState]\n",
    "Action = Tuple[int, int, int]\n",
    "\n",
    "@dataclass\n",
    "class TwoStoreInventory(fMDP[State, Action]):\n",
    "    \"\"\"Representation of the Two-Store Inventory Control problem.\"\"\"\n",
    "    \n",
    "    capacities: Tuple[int, int]\n",
    "    lambdas: Tuple[float, float]\n",
    "    holding_costs: Tuple[float, float]\n",
    "    stockout_costs: Tuple[float, float]\n",
    "    supplier_cost: float\n",
    "    transfer_cost: float\n",
    "        \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Instantiate base class with proper distributions.\"\"\"\n",
    "        super().__init__(mapping=self._map_transitions())\n",
    "        \n",
    "    def _get_probability_reward(\n",
    "        self, \n",
    "        s: State, \n",
    "        a: Action, \n",
    "        x: int\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"Compute joint probability of this transition.\"\"\"\n",
    "        delta: int = a[1]\n",
    "            \n",
    "        def _prob_reward(i: int) -> float:\n",
    "            \"\"\"Compute probability of single store MDP transition.\"\"\"\n",
    "            _map = {0: 1, 1: -1}\n",
    "            indicator = _map[i]\n",
    "            Poiss = poisson(self.lambdas[i])\n",
    "            ip = s[i].inventory_position()\n",
    "            if ip - delta * indicator >= x[i]:\n",
    "                p = Poiss.pmf(x[i])\n",
    "                r = -self.h[i] * s[i].on_hand\n",
    "                return p, r\n",
    "            p = Poiss.cdf(x[i])\n",
    "            r = -self.h[i] * s[i].on_hand\n",
    "            r -= self.p[i] * self.lambdas[i] * (1 - Poiss(ip - 1))\n",
    "            r -= ip * (1 - Poiss(ip))\n",
    "            return p, r\n",
    "\n",
    "        P, R = _prob_reward(0)\n",
    "        P_, R_ = _prob_reward(1)\n",
    "        return P * P_, R + R_\n",
    "      \n",
    "    \n",
    "    def _orders_filled(self, _state: State, _action: Action) -> Tuple[int, int]:\n",
    "        \"\"\"We can create next states from enumeration of filled orders.\"\"\"\n",
    "        delta: int = _action[1]\n",
    "\n",
    "        def _orders(i: int) -> int:\n",
    "            \"\"\"Possible orders for single store.\"\"\"\n",
    "            _map = {0: 1, 1: -1}\n",
    "            indicator = _map[i]\n",
    "            ip: int = _state[i].inventory_position()\n",
    "            total_sellable: int = ip - delta * indicator\n",
    "            for j in range(total_sellable + 1):\n",
    "                yield j\n",
    "\n",
    "        for a, b in it.product(_orders(0), _orders(1)):\n",
    "            yield a, b \n",
    "                \n",
    "    def _map_transitions(self) -> StateActionMapping[State, Action]:\n",
    "        \"\"\"Construct the transition probability table.\"\"\"\n",
    "        StateRewardPair = Tuple[State, float]\n",
    "        mapping: Dict[State, Dict[Action, Categorical[StateRewardPair]]] = {}\n",
    "                    \n",
    "        for state in self.all_states:\n",
    "            action_to_dist: Dict[Action, Categorical[StateRewardPair]] = {}\n",
    "            for action in self.available_actions(state):\n",
    "                state_reward_to_prob: Dict[StateRewardPair, float] = {}\n",
    "                for demand in self._orders_filled(state, action):\n",
    "                    P, R = self._get_probability_reward(state, action, demand)\n",
    "                    state_ = self._construct_next_state(state, action, demand)\n",
    "                    state_reward_to_prob[(state_, R)] = P\n",
    "                action_to_dist[action] = Categorical(state_reward_to_prob)\n",
    "            mapping[state] = action_to_dist\n",
    "        return mapping\n",
    "    \n",
    "    def _construct_next_state(\n",
    "        self,\n",
    "        _state: State, \n",
    "        _action: Action, \n",
    "        _demand: Tuple[int, int]\n",
    "    ) -> State:\n",
    "        \"\"\"Form the next state.\"\"\"\n",
    "        delta: int = _action[1]\n",
    "        x = InventoryState(\n",
    "            _state[0].inventory_position() - delta - _demand[0],\n",
    "            _action[0]\n",
    "        )\n",
    "        y = InventoryState(\n",
    "            _state[1].inventory_position() + delta - _demand[1],\n",
    "            _action[2]\n",
    "        )\n",
    "        return x, y\n",
    "                \n",
    "    @property\n",
    "    def cap(self) -> Tuple[int, int]:\n",
    "        \"\"\"Short alias.\"\"\"\n",
    "        return self.capacities\n",
    "    \n",
    "    @property\n",
    "    def h(self) -> Tuple[float, float]:\n",
    "        \"\"\"Short alias.\"\"\"\n",
    "        return self.holding_costs\n",
    "    \n",
    "    @property\n",
    "    def p(self) -> Tuple[float, float]:\n",
    "        \"\"\"Short alias.\"\"\"\n",
    "        return self.stockout_costs\n",
    "\n",
    "    @property\n",
    "    def all_states(self) -> Iterable[State]:\n",
    "        \"\"\"Yields all states of this MDP.\"\"\"\n",
    "        \n",
    "        def _states(i: int) -> Iterable[InventoryState]:\n",
    "            \"\"\"Generate all states given a single store capacity.\"\"\"\n",
    "            for alpha in range(self.cap[i] + 1):\n",
    "                for beta in range(self.cap[i] + 1 - alpha):\n",
    "                    yield InventoryState(alpha, beta)\n",
    "                    \n",
    "        for a, b in it.product(_states(0), _states(1)):\n",
    "            yield a, b\n",
    "            \n",
    "    def available_actions(self, s: State) -> Iterable[Action]:\n",
    "        \"\"\"Given a current state, yield possible next actions.\"\"\"\n",
    "        ips = s[0].inventory_position(), s[1].inventory_position()\n",
    "        n_moveable_up = min(s[0].on_hand, self.cap[1] - ips[1])\n",
    "        n_moveable_down = min(s[1].on_hand, self.cap[0] - ips[0])\n",
    "        \n",
    "        for transfer in range(-n_moveable_down, n_moveable_up + 1):\n",
    "            for a in range(self.cap[0] - ips[0] + transfer + 1):\n",
    "                for b in range(self.cap[1] - ips[1] - transfer + 1):\n",
    "                    msg = f\"{s}, {transfer=}\"\n",
    "                    assert transfer <= s[0].on_hand, msg\n",
    "                    assert ips[0] - transfer <= self.cap[0], msg\n",
    "                    assert -transfer <= s[1].on_hand, msg\n",
    "                    assert ips[1] + transfer <= self.cap[1], msg\n",
    "                    yield a, transfer, b\n",
    "                \n",
    "                      \n",
    "    def possible_next_states(s: State, a: Action) -> Iterable[State]:\n",
    "        \"\"\"Yield all possible next states for a given s, a pair.\"\"\"\n",
    "        \n",
    "        def _single_next_states(on_hand: int, ordered: int) -> InventoryState:\n",
    "            \"\"\"Account for all possible daily demands.\"\"\"\n",
    "            for demand in range(on_hand + 1):\n",
    "                yield InventoryState(on_hand - demand, ordered)\n",
    "        \n",
    "        order0, transfer, order1 = a\n",
    "        morning_inventory0 = s[0].inventory_position() - transfer\n",
    "        morning_inventory1 = s[1].inventory_position() + transfer\n",
    "        for a, b in it.product(\n",
    "            _next_states(morning_inventory0, order0),\n",
    "            _next_states(morning_inventory1, order1)\n",
    "        ):\n",
    "            yield a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = TwoStoreInventory(\n",
    "    capacities=(3, 2),\n",
    "    lambdas=(1, 3),\n",
    "    holding_costs=(2, 3),\n",
    "    stockout_costs=(7, 6),\n",
    "    supplier_cost=8,\n",
    "    transfer_cost=2,\n",
    ")\n",
    "opt_vf, opt_policy = value_iteration_result(mdp, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=0)): 0.0,\n",
       " (InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=1)): -0.8709674523163493,\n",
       " (InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=2)): -2.0857377676002486,\n",
       " (InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=0)): -3.870967452316349,\n",
       " (InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=1)): -5.085737767600248,\n",
       " (InventoryState(on_hand=0, on_order=0),\n",
       "  InventoryState(on_hand=2, on_order=0)): -8.08573776760025,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=0)): -1.2919349046326987,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=1)): -2.2015801042334386,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=2)): -3.4588611763192936,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=0)): -5.201580104233439,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=1)): -6.458861176319293,\n",
       " (InventoryState(on_hand=0, on_order=1),\n",
       "  InventoryState(on_hand=2, on_order=0)): -9.458861176319294,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=0)): -3.224412562428527,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=1)): -4.1715160760366405,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=2)): -5.494957319406379,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=1, on_order=0)): -7.171516076036641,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=1, on_order=1)): -8.49495731940638,\n",
       " (InventoryState(on_hand=0, on_order=2),\n",
       "  InventoryState(on_hand=2, on_order=0)): -11.49495731940638,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=0, on_order=0)): -5.730774953307169,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=0, on_order=1)): -6.7569231260883305,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=0, on_order=2)): -8.203691392297035,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=1, on_order=0)): -9.756923126088331,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=1, on_order=1)): -11.203691392297033,\n",
       " (InventoryState(on_hand=0, on_order=3),\n",
       "  InventoryState(on_hand=2, on_order=0)): -14.203691392297035,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=0)): -2.870967452316349,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=1)): -4.085737767600249,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=2)): -5.458861176319293,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=0)): -7.085737767600248,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=1)): -8.458861176319294,\n",
       " (InventoryState(on_hand=1, on_order=0),\n",
       "  InventoryState(on_hand=2, on_order=0)): -11.458861176319292,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=0)): -4.201580104233439,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=1)): -5.458861176319293,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=2)): -7.494957319406379,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=0)): -8.458861176319294,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=1)): -10.494957319406378,\n",
       " (InventoryState(on_hand=1, on_order=1),\n",
       "  InventoryState(on_hand=2, on_order=0)): -13.494957319406382,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=0)): -6.17151607603664,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=1)): -7.494957319406379,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=0, on_order=2)): -10.203691392297033,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=1, on_order=0)): -10.494957319406378,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=1, on_order=1)): -13.203691392297037,\n",
       " (InventoryState(on_hand=1, on_order=2),\n",
       "  InventoryState(on_hand=2, on_order=0)): -16.20369139229703,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=0)): -6.085737767600248,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=1)): -7.458861176319292,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=2)): -9.49495731940638,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=0)): -10.458861176319294,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=1)): -12.494957319406378,\n",
       " (InventoryState(on_hand=2, on_order=0),\n",
       "  InventoryState(on_hand=2, on_order=0)): -15.494957319406378,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=0)): -7.458861176319292,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=1)): -9.49495731940638,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=0, on_order=2)): -12.203691392297033,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=0)): -12.494957319406378,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=1, on_order=1)): -15.203691392297031,\n",
       " (InventoryState(on_hand=2, on_order=1),\n",
       "  InventoryState(on_hand=2, on_order=0)): -18.20369139229703,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=0)): -9.458861176319294,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=1)): -11.49495731940638,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=0, on_order=2)): -14.203691392297035,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=0)): -14.494957319406382,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=1, on_order=1)): -17.20369139229703,\n",
       " (InventoryState(on_hand=3, on_order=0),\n",
       "  InventoryState(on_hand=2, on_order=0)): -20.203691392297035}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=0), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=1), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=2), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=0, on_order=3), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=0), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=1), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=1, on_order=2), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 2, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=0), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 2, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=2, on_order=1), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=0, on_order=0)):\n",
       "  Do Action (0, 2, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=0, on_order=1)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=0, on_order=2)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=1, on_order=0)):\n",
       "  Do Action (0, 1, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=1, on_order=1)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000\n",
       "For State (InventoryState(on_hand=3, on_order=0), InventoryState(on_hand=2, on_order=0)):\n",
       "  Do Action (0, 0, 0) with Probability 1.000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
