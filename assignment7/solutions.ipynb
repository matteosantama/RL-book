{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241 (Winter 2021) -- Assignment 7\n",
    "\n",
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Callable, Tuple, Iterator, TypeVar, Generic\n",
    "from pprint import pprint\n",
    "\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Gaussian, Choose, SampledDistribution\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, Policy\n",
    "from rl.function_approx import DNNSpec, AdamGradient, DNNApprox\n",
    "from rl.approximate_dynamic_programming import back_opt_vf_and_policy\n",
    "from rl.approximate_dynamic_programming import back_opt_qvf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we relax the requirement that `risky_return_distributions` can be any generic distributions, and instead impose a certain type of distribution, then we can drastically speed up our program. This is due to the fact we can now directly compute the wealth distribution $W_{t+1}$ and thus performing each `step()` call will no longer need to generate thousands of samples. \n",
    "\n",
    "To do this, we introduce a custom distribution `WealthRewardDistribution` that computes expectations in $O(1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Tuple[float, float]\n",
    "\n",
    "class WealthRewardGaussian(SampledDistribution[T]):\n",
    "    \"\"\"TODO\"\"\"\n",
    "        \n",
    "    def __init__(self, mu: float, sigma: float, reward: float) -> None:\n",
    "        self.mu: float = mu\n",
    "        self.sigma: float = sigma\n",
    "        self.reward: float = reward\n",
    "            \n",
    "        _sampler = lambda: (\n",
    "            np.random.normal(loc=self.mu, scale=self.sigma), self.reward\n",
    "        )\n",
    "        super().__init__(\n",
    "            sampler=_sampler,\n",
    "            expectation_samples=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class AssetAllocDiscrete:\n",
    "    risky_return_distributions: Sequence[Gaussian[float]]\n",
    "    riskless_returns: Sequence[float]\n",
    "    utility_func: Callable[[float], float]\n",
    "    risky_alloc_choices: Sequence[float]\n",
    "    feature_functions: Sequence[Callable[[Tuple[float, float]], float]]\n",
    "    dnn_spec: DNNSpec\n",
    "    initial_wealth_distribution: Gaussian[float]\n",
    "\n",
    "    def time_steps(self) -> int:\n",
    "        return len(self.risky_return_distributions)\n",
    "\n",
    "    def uniform_actions(self) -> Choose[float]:\n",
    "        return Choose(set(self.risky_alloc_choices))\n",
    "\n",
    "    def get_mdp(self, t: int) -> MarkovDecisionProcess[float, float]:\n",
    "        \"\"\"\n",
    "        State is Wealth W_t, Action is investment in risky asset (= x_t)\n",
    "        Investment in riskless asset is W_t - x_t\n",
    "        \"\"\"\n",
    "\n",
    "        distr: Gaussian[float] = self.risky_return_distributions[t]\n",
    "        rate: float = self.riskless_returns[t]\n",
    "        alloc_choices: Sequence[float] = self.risky_alloc_choices\n",
    "        steps: int = self.time_steps()\n",
    "        utility_f: Callable[[float], float] = self.utility_func\n",
    "\n",
    "        class AssetAllocMDP(MarkovDecisionProcess[float, float]):\n",
    "            def step(self, wealth: float, alloc: float) -> WealthRewardGaussian:\n",
    "                expected_wealth = alloc * distr.μ + (wealth - alloc) * (1 + rate) \n",
    "                reward: float = utility_f(expected_wealth) if t == steps - 1 else 0.0    \n",
    "                wealth = WealthRewardGaussian(\n",
    "                    mu=expected_wealth,\n",
    "                    sigma=distr.σ,\n",
    "                    reward=reward\n",
    "                )\n",
    "                return wealth\n",
    "\n",
    "            def actions(self, wealth: float) -> Sequence[float]:\n",
    "                return alloc_choices\n",
    "\n",
    "        return AssetAllocMDP()\n",
    "\n",
    "    def get_qvf_func_approx(self) -> DNNApprox[Tuple[float, float]]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1, decay1=0.9, decay2=0.999\n",
    "        )\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=self.feature_functions,\n",
    "            dnn_spec=self.dnn_spec,\n",
    "            adam_gradient=adam_gradient,\n",
    "        )\n",
    "\n",
    "    def get_states_distribution(self, t: int) -> SampledDistribution[float]:\n",
    "\n",
    "        actions_distr: Choose[float] = self.uniform_actions()\n",
    "\n",
    "        def states_sampler_func() -> float:\n",
    "            wealth: float = self.initial_wealth_distribution.sample()\n",
    "            for i in range(t):\n",
    "                distr: Distribution[float] = self.risky_return_distributions[i]\n",
    "                rate: float = self.riskless_returns[i]\n",
    "                alloc: float = actions_distr.sample()\n",
    "                wealth = alloc * (1 + distr.sample()) + (wealth - alloc) * (1 + rate)\n",
    "            return wealth\n",
    "\n",
    "        return SampledDistribution(states_sampler_func)\n",
    "\n",
    "    def backward_induction_qvf(self) -> Iterator[DNNApprox[Tuple[float, float]]]:\n",
    "\n",
    "        init_fa: DNNApprox[Tuple[float, float]] = self.get_qvf_func_approx()\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[\n",
    "            Tuple[\n",
    "                MarkovDecisionProcess[float, float],\n",
    "                DNNApprox[Tuple[float, float]],\n",
    "                SampledDistribution[float],\n",
    "            ]\n",
    "        ] = [\n",
    "            (self.get_mdp(i), init_fa, self.get_states_distribution(i))\n",
    "            for i in range(self.time_steps())\n",
    "        ]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-6\n",
    "\n",
    "        return back_opt_qvf(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance,\n",
    "        )\n",
    "\n",
    "    def get_vf_func_approx(\n",
    "        self, ff: Sequence[Callable[[float], float]]\n",
    "    ) -> DNNApprox[float]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1, decay1=0.9, decay2=0.999\n",
    "        )\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=ff, dnn_spec=self.dnn_spec, adam_gradient=adam_gradient\n",
    "        )\n",
    "\n",
    "    def backward_induction_vf_and_pi(\n",
    "        self, ff: Sequence[Callable[[float], float]]\n",
    "    ) -> Iterator[Tuple[DNNApprox[float], Policy[float, float]]]:\n",
    "\n",
    "        init_fa: DNNApprox[float] = self.get_vf_func_approx(ff)\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[\n",
    "            Tuple[\n",
    "                MarkovDecisionProcess[float, float],\n",
    "                DNNApprox[float],\n",
    "                SampledDistribution[float],\n",
    "            ]\n",
    "        ] = [\n",
    "            (self.get_mdp(i), init_fa, self.get_states_distribution(i))\n",
    "            for i in range(self.time_steps())\n",
    "        ]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-8\n",
    "\n",
    "        return back_opt_vf_and_policy(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps: int = 4\n",
    "μ: float = 0.13\n",
    "σ: float = 0.2\n",
    "r: float = 0.07\n",
    "a: float = 1.0\n",
    "init_wealth: float = 1.0\n",
    "init_wealth_var: float = 0.1\n",
    "\n",
    "excess: float = μ - r\n",
    "var: float = σ * σ\n",
    "base_alloc: float = excess / (a * var)\n",
    "\n",
    "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "utility_function: Callable[[float], float] = lambda x: -np.exp(-a * x) / a\n",
    "alloc_choices: Sequence[float] = np.linspace(\n",
    "    2 / 3 * base_alloc, 4 / 3 * base_alloc, 11\n",
    ")\n",
    "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = [\n",
    "    lambda _: 1.0,\n",
    "    lambda w_x: w_x[0],\n",
    "    lambda w_x: w_x[1],\n",
    "    lambda w_x: w_x[1] * w_x[1],\n",
    "]\n",
    "dnn: DNNSpec = DNNSpec(\n",
    "    neurons=[],\n",
    "    bias=False,\n",
    "    hidden_activation=lambda x: x,\n",
    "    hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "    output_activation=lambda x: -np.sign(a) * np.exp(-x),\n",
    "    output_activation_deriv=lambda y: -y,\n",
    ")\n",
    "init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_var)\n",
    "\n",
    "aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "    risky_return_distributions=risky_ret,\n",
    "    riskless_returns=riskless_ret,\n",
    "    utility_func=utility_function,\n",
    "    risky_alloc_choices=alloc_choices,\n",
    "    feature_functions=feature_funcs,\n",
    "    dnn_spec=dnn,\n",
    "    initial_wealth_distribution=init_wealth_distr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Induction on Q-Value Function\n",
      "--------------------------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 1.000, Opt Val = -19.233\n",
      "Optimal Weights below:\n",
      "array([[-3.200457  ,  1.33488506, -1.06969302, -0.02136637]])\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 1.000, Opt Val = -6.337\n",
      "Optimal Weights below:\n",
      "array([[-1.85315566,  1.22131011, -1.283878  ,  0.06933296]])\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 1.000, Opt Val = -2.269\n",
      "Optimal Weights below:\n",
      "array([[-0.94338294,  1.15439029, -1.04392853,  0.01341401]])\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 1.000, Opt Val = -0.878\n",
      "Optimal Weights below:\n",
      "array([[-4.56861237e-04,  1.07000039e+00, -9.39387913e-01,\n",
      "        -1.97487816e-04]])\n",
      "\n",
      "Analytical Solution\n",
      "-------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 1.224, Opt Val = -0.225\n",
      "Bias Weight = 0.135\n",
      "W_t Weight = 1.311\n",
      "x_t Weight = 0.074\n",
      "x_t^2 Weight = -0.030\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 1.310, Opt Val = -0.257\n",
      "Bias Weight = 0.090\n",
      "W_t Weight = 1.225\n",
      "x_t Weight = 0.069\n",
      "x_t^2 Weight = -0.026\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 1.402, Opt Val = -0.291\n",
      "Bias Weight = 0.045\n",
      "W_t Weight = 1.145\n",
      "x_t Weight = 0.064\n",
      "x_t^2 Weight = -0.023\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 1.500, Opt Val = -0.328\n",
      "Bias Weight = 0.000\n",
      "W_t Weight = 1.070\n",
      "x_t Weight = 0.060\n",
      "x_t^2 Weight = -0.020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it_qvf: Iterator[DNNApprox[Tuple[float, float]]] = aad.backward_induction_qvf()\n",
    "\n",
    "print(\"Backward Induction on Q-Value Function\")\n",
    "print(\"--------------------------------------\")\n",
    "print()\n",
    "for t, q in enumerate(it_qvf):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    opt_alloc: float = max(\n",
    "        ((q.evaluate([(init_wealth, ac)])[0], ac) for ac in alloc_choices),\n",
    "        key=itemgetter(0),\n",
    "    )[1]\n",
    "    val: float = max(q.evaluate([(init_wealth, ac)])[0] for ac in alloc_choices)\n",
    "    print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "    print(\"Optimal Weights below:\")\n",
    "    for wts in q.weights:\n",
    "        pprint(wts.weights)\n",
    "    print()\n",
    "\n",
    "print(\"Analytical Solution\")\n",
    "print(\"-------------------\")\n",
    "print()\n",
    "\n",
    "for t in range(steps):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    left: int = steps - t\n",
    "    growth: float = (1 + r) ** (left - 1)\n",
    "    alloc: float = base_alloc / growth\n",
    "    val: float = (\n",
    "        -np.exp(\n",
    "            -excess * excess * left / (2 * var) - a * growth * (1 + r) * init_wealth\n",
    "        )\n",
    "        / a\n",
    "    )\n",
    "    bias_wt: float = excess * excess * (left - 1) / (2 * var) + np.log(np.abs(a))\n",
    "    w_t_wt: float = a * growth * (1 + r)\n",
    "    x_t_wt: float = a * excess * growth\n",
    "    x_t2_wt: float = -var * (a * growth) ** 2 / 2\n",
    "\n",
    "    print(f\"Opt Risky Allocation = {alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "    print(f\"Bias Weight = {bias_wt:.3f}\")\n",
    "    print(f\"W_t Weight = {w_t_wt:.3f}\")\n",
    "    print(f\"x_t Weight = {x_t_wt:.3f}\")\n",
    "    print(f\"x_t^2 Weight = {x_t2_wt:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
