{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 241 -- Assignment 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import operator\n",
    "import itertools as it\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.markov_decision_process import (\n",
    "    FiniteMarkovDecisionProcess as FiniteMDP, \n",
    "    FinitePolicy,\n",
    "    TransitionStep\n",
    ")\n",
    "from rl.chapter3.simple_inventory_mdp_cap import (\n",
    "    SimpleInventoryMDPCap, \n",
    "    InventoryState\n",
    ")\n",
    "from rl.distribution import Constant, Choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_G(\n",
    "    episode: List[TransitionStep]\n",
    ") -> Dict[Tuple[InventoryState, int], float]:\n",
    "    \"\"\"Compute total return G for each state, action pair.\"\"\"\n",
    "    G = {}\n",
    "    for i, step in enumerate(episode):\n",
    "        pair = (step.state, step.action)\n",
    "        if pair not in G:\n",
    "            G[pair] = sum(x.reward for x in episode[i:])\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_policy(\n",
    "    mdp: FiniteMDP[InventoryState, int],\n",
    "    Q: Dict[Tuple[InventoryState, int], float]\n",
    ") -> FinitePolicy[InventoryState, int]:\n",
    "    \"\"\"Construct the greedy policy from the Q values.\"\"\"\n",
    "    mapping = {}\n",
    "    for state in mdp.states():\n",
    "        if not mdp.is_terminal(state):\n",
    "            actions = filter(lambda x: x[0] == state, Q)\n",
    "            greedy = max(actions, key=lambda x: Q[x])[1]\n",
    "            mapping[state] = Constant(greedy)\n",
    "        else:\n",
    "            mapping[state] = None\n",
    "            \n",
    "    return FinitePolicy(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_control(\n",
    "    mdp: FiniteMDP[InventoryState, int]\n",
    ") -> FinitePolicy[InventoryState, int]:\n",
    "    \"\"\"Find optimal policy by iteratively evaluating candidates.\"\"\"\n",
    "    Q = {\n",
    "        (s, a): np.random.randn() \n",
    "        for s in mdp.non_terminal_states\n",
    "        for a in mdp.actions(s)\n",
    "    }\n",
    "    \n",
    "    policy_map = {}\n",
    "    for state in mdp.states():\n",
    "        if mdp.is_terminal(state):\n",
    "            policy_map[state] = None\n",
    "        else:\n",
    "            action = np.random.choice(list(mdp.actions(state)))\n",
    "            policy_map[state] = Constant(action)\n",
    "    policy = FinitePolicy(policy_map)\n",
    "    \n",
    "    returns = defaultdict(list)\n",
    "    \n",
    "    starts = Choose({\n",
    "        s: 1 / len(mdp.states()) \n",
    "        for s in mdp.non_terminal_states\n",
    "    })\n",
    "    for k in range(100):\n",
    "        episode = list(it.islice(mdp.simulate_actions(starts, policy), 1009))\n",
    "        G = compute_G(episode)\n",
    "        for pair in G:\n",
    "            returns[pair].append(G[pair])\n",
    "            Q[pair] = sum(returns[pair]) / len(returns[pair])\n",
    "        policy = get_greedy_policy(mdp, Q)\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = SimpleInventoryMDPCap(\n",
    "    capacity=2,\n",
    "    poisson_lambda=1.0,\n",
    "    holding_cost=1.0,\n",
    "    stockout_cost=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State InventoryState(on_hand=0, on_order=0):\n",
       "  Do Action 1 with Probability 1.000\n",
       "For State InventoryState(on_hand=0, on_order=1):\n",
       "  Do Action 1 with Probability 1.000\n",
       "For State InventoryState(on_hand=0, on_order=2):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State InventoryState(on_hand=1, on_order=0):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State InventoryState(on_hand=1, on_order=1):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State InventoryState(on_hand=2, on_order=0):\n",
       "  Do Action 0 with Probability 1.000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcc_policy = monte_carlo_control(mdp)\n",
    "mcc_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
